{"url_object_id": "9d318dfaa9e2e87c3a9c89f18682922d", "title": "分布式之延时任务方案解析", "url": "http://blog.jobbole.com/114009/", "create_date": "2018/05/20", "praise_nums": "2", "favor_nums": 1, "comment_nums": 0, "content": "在开发中，往往会遇到一些关于延时任务的需求。例如生成订单30分钟未支付，则自动取消生成订单60秒后,给用户发短信对上述的任务，我们给一个专业的名字来形容，那就是延时任务。那么这里就会产生一个问题，这个延时任务和定时任务的区别究竟在哪里呢？一共有如下几点区别下面，我们以判断订单是否超时为例，进行方案分析(1)数据库轮询该方案通常是在小型项目中使用，即通过一个线程定时的去扫描数据库，通过订单时间来判断是否有超时的订单，然后进行update或delete等操作博主当年早期是用quartz来实现的(实习那会的事)，简单介绍一下\nmaven项目引入一个依赖如下所示    <dependency>        <artifactId>quartz</artifactId>    </dependency>调用Demo类MyJob如下所示package com.rjzheng.delay1;import org.quartz.JobBuilder;import org.quartz.Scheduler;import org.quartz.SchedulerFactory;import org.quartz.Trigger;import org.quartz.impl.StdSchedulerFactory;import org.quartz.JobExecutionContext;    public void execute(JobExecutionContext context)        System.out.println(\"要去数据库扫描啦。。。\");        // 创建任务                .withIdentity(\"job1\", \"group1\").build();        Trigger trigger = TriggerBuilder                .withIdentity(\"trigger1\", \"group3\")                        SimpleScheduleBuilder.simpleSchedule()                .build();        // 将任务及其触发器放入调度器        // 调度器开始调度任务    }运行代码，可发现每隔3秒，输出如下优点:简单易行，支持集群操作\n缺点:(1)对服务器内存消耗大\n(2)存在延迟，比如你每隔3分钟扫描一次，那最坏的延迟时间就是3分钟\n(3)假设你的订单有几千万条，每隔几分钟这样扫描一次，数据库损耗极大(2)JDK的延迟队列该方案是利用JDK自带的DelayQueue来实现，这是一个无界阻塞队列，该队列只有在延迟期满的时候才能从中获取元素，放入DelayQueue中的对象，是必须实现Delayed接口的。\nDelayedQueue实现工作流程如下图所示其中Poll():获取并移除队列的超时元素，没有则返回空\ntake():获取并移除队列的超时元素，如果没有则wait当前线程，直到有元素满足超时条件，返回结果。定义一个类OrderDelay实现Delayed，代码如下package com.rjzheng.delay2;import java.util.concurrent.Delayed;        private long timeout;    OrderDelay(String orderId, long timeout) {        this.timeout = timeout + System.nanoTime();        if (other == this)        OrderDelay t = (OrderDelay) other;                .getDelay(TimeUnit.NANOSECONDS));    }    // 返回距离你自定义的超时时间还有多少        return unit.convert(timeout - System.nanoTime(), TimeUnit.NANOSECONDS);        System.out.println(orderId+\"编号的订单要删除啦。。。。\");}运行的测试Demo为，我们设定延迟时间为3秒package com.rjzheng.delay2;import java.util.ArrayList;import java.util.concurrent.DelayQueue;     public static void main(String[] args) {              List<String> list = new ArrayList<String>();              list.add(\"00000002\");              list.add(\"00000004\");              DelayQueue<OrderDelay> queue = new DelayQueue<OrderDelay>();              for(int i = 0;i<5;i++){                  queue.put(new OrderDelay(list.get(i),                      try {                           System.out.println(\"After \" +                   } catch (InterruptedException e) {                      e.printStackTrace();              }      输出如下0000000100000002000000030000000400000005可以看到都是延迟3秒，订单被删除优点:效率高,任务触发时间延迟低。\n缺点:(1)服务器重启后，数据全部消失，怕宕机\n(2)集群扩展相当麻烦\n(3)因为内存条件限制的原因，比如下单未付款的订单数太多，那么很容易就出现OOM异常\n(4)代码复杂度较高(3)时间轮算法先上一张时间轮的图(这图到处都是啦)时间轮算法可以类比于时钟，如上图箭头（指针）按某一个方向按固定频率轮动，每一次跳动称为一个 tick。这样可以看出定时轮由个3个重要的属性参数，ticksPerWheel（一轮的tick数），tickDuration（一个tick的持续时间）以及 timeUnit（时间单位），例如当ticksPerWheel=60，tickDuration=1，timeUnit=秒，这就和现实中的始终的秒针走动完全类似了。如果当前指针指在1上面，我有一个任务需要4秒以后执行，那么这个执行的线程回调或者消息将会被放在5上。那如果需要在20秒之后执行怎么办，由于这个环形结构槽数只到8，如果要20秒，指针需要多转2圈。位置是在2圈之后的5上面（20 % 8 + 1）我们用Netty的HashedWheelTimer来实现\n给Pom加上下面的依赖        <dependency>            <artifactId>netty-all</artifactId>        </dependency>测试代码HashedWheelTimerTest如下所示package com.rjzheng.delay3;import io.netty.util.HashedWheelTimer;import io.netty.util.Timer;    static class MyTimerTask implements TimerTask{        public MyTimerTask(boolean flag){        }            // TODO Auto-generated method stub             this.flag =false;    }        MyTimerTask timerTask = new MyTimerTask(true);        timer.newTimeout(timerTask, 5, TimeUnit.SECONDS);        while(timerTask.flag){                Thread.sleep(1000);                // TODO Auto-generated catch block            }            i++;    }输出如下1356优点:效率高,任务触发时间延迟时间比delayQueue低，代码复杂度比delayQueue低。\n缺点:(1)服务器重启后，数据全部消失，怕宕机\n(2)集群扩展相当麻烦\n(3)因为内存条件限制的原因，比如下单未付款的订单数太多，那么很容易就出现OOM异常(4)redis缓存利用redis的zset,zset是一个有序集合，每一个元素(member)都关联了一个score,通过score排序来取集合中的值\nzset常用命令\n添加元素:ZADD key score member [[score member] [score member] …]\n按顺序查询元素:ZRANGE key start stop [WITHSCORES]\n查询元素score:ZSCORE key member\n移除元素:ZREM key member [member …]\n测试如下# 添加单个元素redis> ZADD page_ 10 google.com# 添加多个元素redis> ZADD page_ 9 baidu.com 8 bing.com1) \"bing.com\"3) \"baidu.com\"5) \"google.com\"redis> ZSCORE page_rank bing.com(integer) 1redis> ZRANGE page_ 0 -1 WITHSCORES2) \"8\"4) \"9\"那么如何实现呢？我们将订单超时时间戳与订单号分别设置为score和member,系统扫描第一个元素判断是否超时，具体如下图所示package com.rjzheng.delay4;import java.util.Calendar;import redis.clients.jedis.JedisPool;    private static final String ADDR = \"127.0.0.1\";    private static JedisPool jedisPool = new JedisPool(ADDR, PORT);    public static Jedis getJedis() {    }    //生产者,生成5个订单放进去        for(int i=0;i<5;i++){            Calendar cal1 = Calendar.getInstance();            int second3later = (int) (cal1.getTimeInMillis() / 1000);            System.out.println(System.currentTimeMillis()+\"ms:redis生成了一个订单任务：订单ID为\"+\"OID0000001\"+i);    }    //消费者，取订单        Jedis jedis = AppTest.getJedis();            Set<Tuple> items = jedis.zrangeWithScores(\"OrderId\", 0, 1);                System.out.println(\"当前没有等待的任务\");                    Thread.sleep(500);                    // TODO Auto-generated catch block                }            }            Calendar cal = Calendar.getInstance();            if(nowSecond >= score){                jedis.zrem(\"OrderId\", orderId);            }    }    public static void main(String[] args) {        appTest.productionDelayMessage();    }}此时对应输出如下1525086085261ms:redisIDOID000000101525086085266ms:redisIDOID000000121525086085270ms:redisIDOID000000141525086088001ms:redisOrderIdOID000000111525086088003ms:redisOrderIdOID00000013可以看到，几乎都是3秒之后，消费订单。然而，这一版存在一个致命的硬伤，在高并发条件下，多消费者会取到同一个订单号，我们上测试代码ThreadTestpackage com.rjzheng.delay4;import java.util.concurrent.CountDownLatch;public class ThreadTest {    private static CountDownLatch cdl = new CountDownLatch(threadNum);        public void run() {                cdl.await();                // TODO Auto-generated catch block            }            appTest.consumerDelayMessage();    }        AppTest appTest =new AppTest();        for(int i=0;i<threadNum;i++){            cdl.countDown();    }输出如下所示1525087157727ms:redisIDOID000000101525087157738ms:redisIDOID000000121525087157753ms:redisIDOID000000141525087160011ms:redisOrderIdOID000000101525087160022ms:redisOrderIdOID000000111525087160029ms:redisOrderIdOID000000111525087160045ms:redisOrderIdOID000000121525087160053ms:redisOrderIdOID000000131525087160065ms:redisOrderIdOID00000014显然，出现了多个线程消费同一个资源的情况。(1)用分布式锁，但是用分布式锁，性能下降了，该方案不细说。\n(2)对ZREM的返回值进行判断，只有大于0的时候，才消费数据，于是将consumerDelayMessage()方法里的if(nowSecond >= score){    jedis.zrem(\"OrderId\", orderId);}修改为if(nowSecond >= score){    Long num = jedis.zrem(\"OrderId\", orderId);        System.out.println(System.currentTimeMillis()+\"ms:redis消费了一个任务：消费的订单OrderId为\"+orderId);}在这种修改后，重新运行ThreadTest类，发现输出正常了该方案使用redis的Keyspace Notifications，中文翻译就是键空间机制，就是利用该机制可以在key失效之后，提供一个回调，实际上是redis会给客户端发送一个消息。是需要redis版本2.8以上。在redis.conf中，加入一条配置notify-keyspace-events Ex运行代码如下package com.rjzheng.delay5;import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisPubSub;public class RedisTest {    private static final int PORT = 6379;    private static RedisSub sub = new RedisSub();    public static void init() {            public void run() {            }    }    public static void main(String[] args) throws InterruptedException {        for(int i =0;i<10;i++){            jedis.getResource().setex(orderId, 3, orderId);        }            <a href='http://www.jobbole.com/members/wx610506454'>@Override</a>            System.out.println(System.currentTimeMillis()+\"ms:\"+message+\"订单取消\");    }输出如下1525096202813ms:OID00000001525096202824ms:OID00000021525096202830ms:OID00000041525096202839ms:OID00000061525096205920ms:OID00000051525096205920ms:OID00000011525096205920ms:OID0000006可以明显看到3秒过后，订单取消了\nps:redis的pub/sub机制存在一个硬伤，官网内容如下\n原:Because Redis Pub/Sub is fire and forget currently there is no way to use this feature if your application demands reliable notification of events, that is, if your Pub/Sub client disconnects, and reconnects later, all the events delivered during the time the client was disconnected are lost.\n翻: Redis的发布/订阅目前是即发即弃(fire and forget)模式的，因此无法实现事件的可靠通知。也就是说，如果发布/订阅的客户端断链之后又重连，则在客户端断链期间的所有事件都丢失了。\n因此，方案二不是太推荐。当然，如果你对可靠性要求不高，可以使用。优点:(1)由于使用Redis作为消息通道，消息都存储在Redis中。如果发送程序或者任务处理程序挂了，重启之后，还有重新处理数据的可能性。\n(2)做集群扩展相当方便\n(3)时间准确度高\n缺点:(1)需要额外进行redis维护(5)使用消息队列我们可以采用rabbitMQ的延时队列。RabbitMQ具有以下两个特性，可以实现延迟队列RabbitMQ可以针对Queue和Message设置 x-message-tt，来控制消息的生存时间，如果超时，则消息变为dead letterlRabbitMQ的Queue可以配置x-dead-letter-exchange 和x-dead-letter-routing-key（可选）两个参数，用来控制队列内出现了deadletter，则按照这两个参数重新路由。\n结合以上两个特性，就可以模拟出延迟消息的功能,具体的，我改天再写一篇文章，这里再讲下去，篇幅太长。优点: 高效,可以利用rabbitmq的分布式特性轻易的进行横向扩展,消息支持持久化增加了可靠性。\n缺点：本身的易用度要依赖于rabbitMq的运维.因为要引用rabbitMq,所以复杂度和成本变高本文总结了目前互联网中，绝大部分的延时任务的实现方案。希望大家在工作中能够有所收获。\n其实大家在工作中，百分九十的人还是以业务逻辑为主，很少有机会能够进行方案设计。所以博主不推荐在分布式这块，花太多时间，应该看看《手把手系列的文章》。不过，鉴于现在的面试造火箭，工作拧螺丝现象太过严重，所以博主开始写《分布式系列》，最后来个小漫画娱乐一下。\n", "author": "孤独烟", "tags": ["IT技术", "分布式", "数据库"]}
{"url_object_id": "559edfe4360defce6adda9e865d11934", "title": "我似乎理解了编程的意义", "url": "http://blog.jobbole.com/112098/", "create_date": "2018/05/21", "praise_nums": "1", "favor_nums": 0, "comment_nums": 1, "content": "编程的意义是什么，我又为什么要编程呢？这是一个不时会浮现在我脑海中的问题，它来得并不频繁，但每次却都伴随着对自己职业生涯或人生目标的质疑而产生，令我感到些许困惑和不安。而在这十几年的职业生涯中，我也似乎总能在每个阶段为自己找到一个继续热爱编程的理由，直到它已无法解答再一次疑惑的产生。就这样一次又一次的循环往复，我似乎渐渐理解了编程的意义……编程是一项技能回想大学毕业刚成为一名程序员时，自己对技术是如此狂热，我不断地购买各类技术书籍，几乎所有的业余时间也都被用来钻研技术，提高自己的编程能力。我也因此很快成了同一批入职新人中，编码效率和质量最突出的一个。而在那段时间里所做的技术积累，也成了我日后工作的坚实基础，编程作为一项技能已经深深地嵌入到了我的身体里。即使到了今天，我仍非常怀念那段心无旁骛，一心钻研技术的日子。我为能在工作中写出的每一行优秀代码而兴奋，更为每一天能在技术上取得的点滴进步而喜悦，一切都是那么单纯，编程的意义对于那时的我来说就在于技术本身。编程是去解决问题“能力越大，责任也也大”，这句电影“蜘蛛侠”中的经典台词同样适用于程序员的职业生涯。随着技术能力的提升以及工作中获得的认可，我的职位也由原来的初级程序员变为了资深开发工程师，以及后来的架构师。相应的，除了编程之外，我工作中的很大一部分时间需要用来与用户进行沟通，并分析他们提出的需求。对于我来说这个角色转换的过程，是艰难甚至有些痛苦的。 我不得不用自己最薄弱的沟通技能去和用户打交道，更要命的是我所习惯使用的那些技术语言有时很难让他们理解。我很快意识到自己已不再是那个只需被动接受任务安排，并将自己的编程工作完成好就万事大吉的初级程序员。除了技术之外，我更需要能够突破程序员思维，去发现用户需求背后所隐含的真正问题。我比以前变得更加务实，不再刻意追求技术的高深，而是尽可能从问题本身出发，选择最有效的技术手段去解决它。此时，编程的意义也发生了改变，它已不再局限于技术本身，而成了解决问题的理想工具。编程是在表达，也是在创作就这样又过了几年，当“为什么要编程？”这个问题再次摆在我的面前时，自己也已过了而立之年。对于大多数中国程序员来说，这个年纪已经算是高龄，甚至还有很多人会认为 30 岁还在编程，一定是混得不够好吧。当然，对于这些质疑我也总是一笑了之。其实，在此之前我也有过很多转型的机会，比如去业务部门，或是转作管理等等，但最终我还是选择留在了技术岗位上，因为我觉得编程仍是我最喜欢的，或许也是我唯一擅长的吧。而这个时期也成了我整个程序员生涯的黄金期，我写了公司的核心框架以及一些重要业务系统的核心算法。我很享受这段时光，因为我已几乎感受不到那些技术上的牵绊，我更像雕刻师使用手中的刻刀一般，自如地运用编程来实现那些我认为优秀的东西。编程对于我来说已不再是一项技能或是工具，我是在通过编程进行着自我表达与创作，这种感受带给了我极大的自由度，而我也从中感受到了前所未有的喜悦与乐趣。编程是为了留下痕迹最终我还是走上了管理岗位，这里面有很多个人无法左右的因素（包括大环境、家庭、经济等等）。但我仍然更乐意被大家称为程序员或者“老”程序员。就像在简书的自我介绍中，我总是把全栈工程师放在那些“头衔”的第一位，我也还在利用业余时间做自己喜欢的开源或个人项目。当我再一次问自己“为什么要编程”时，获得了与以往不一样的感悟：或许我们编程是为了能够留下一些痕迹吧。公司里最近都在为一个老系统的升级问题发愁，这个系统已经运行了将近 20 年时间了，为了升级系统，大家不得不深入到这个系统的框架中，去读底层代码。我们读到了一位已经退休的美国同事Bill所实现的数据库连接池代码。在那个时候JAVA刚开始流行，还没有像 Spring 这样的框架，或是如 Hibernate 或 MyBatis 这样标准的持久层实现，这个系统中所有的数据库连接池及核心持久层代码都是由我的这位美国同事写的，这些代码让整个系统稳定运行了将近20年，大家都不禁为他高超的技术水平发出由衷的赞叹。我还认识一位从事证券交易软件研发的公司 CTO，看年纪应该已经接近 50 了，但他仍然在亲自写着那些证券交易的核心代码。当我问他到了这个年龄和职位，为什么还要坚持写代码时，他告诉我，当他看到自己所写的代码每天在支撑着千亿级的证券交易时，他感到非常兴奋和自豪，并不断地希望能够通过自己的努力将它做得更好。我的这个美国同事不会听到大家为他十几年前所代码发出的zan叹，股民们也不会知道这位 CTO 所写的代码正在支撑着他们的日常交易。那些优秀的代码是他们留下的痕迹，我们不能确定这些痕迹能够保留多久，或许几年，或许更短，但它们都曾经在我们的日常生活中产生了重要的价值，而新的未来也将构建在这些痕迹的基础之上，我想这可能才是编程的意义所在吧。我似乎理解了编程的意义，但我明白未来的某一天，我一定还会问自己同样的问题——为什么要编程，希望到那个时候自己还能是那个热爱编程，有着一颗匠心的“技匠”吧……", "author": "技匠（微信公众号：techmask）", "tags": ["职场", " 1 评论 ", "程序员", "职场"]}
{"url_object_id": "4138e1905de02770bdfcad265c9040f4", "title": "如何高效学习", "url": "http://blog.jobbole.com/114034/", "create_date": "2018/05/22", "praise_nums": "1", "favor_nums": 3, "comment_nums": 0, "content": "IT 行业是一个变化非常快的行业，它需要我们持续去学习新的知识和技能。 但是，工作以后，我们经常会发现自己学习的东西很少了，倒不是没有时间去学习， 而是学习的效率太低了。久而久之，就演变成『一年的工作经验，重复用十年』。当然，有些人会说自己经常加班，没有时间学习，这只是表象，时间挤挤总是有的。 你想想你为了上王者，浪费了多少时间？为了刷今日头条，又消磨了多少光阴？另外，很多人推崇碎片化学习，但是有一些东西碎片化学习效率是很低的，比如数学。这篇文章是我学习完 coursera 上面的《Learning How to Learn》MOOC加上我自己多年来的学习经验积累整理而来。注：文中可能有一些内容思考没有很深入，另外一些观点可能还需要更多的时间去检验，读者请自行甄别。 ", "author": "子龙山人", "tags": ["职场", "学习"]}
{"url_object_id": "cda2e6399ff30560f3a49b7fb8c7280a", "title": "是什么让初级工程师走投无路？", "url": "http://blog.jobbole.com/113954/", "create_date": "2018/05/24", "praise_nums": "1", "favor_nums": 0, "comment_nums": 0, "content": "几个月前，我参加了一场针对技术领域女性的活动。很多参加者中是新的开发者，毕业于编程学校或者计算机科学课程。几乎所有人都告诉我，她们在获得第一份工作时遇到了麻烦。我很幸运。我在大学的第一份“真正”工作是 2010 年哥伦比亚大学的“初级应用程序开发人员”。现如今，甚至找不到一个招聘初级开发者岗位的招聘帖。发这些招聘帖的人说他们被淹没在了简历中。然而优秀的公司又抱怨找不到好的工程师。我想知道这是为什么？我不知道这样做，具体来说能够为我们节省多少成本，毕竟我不参与公司的运营。但是我知道很多公司对我说过：「我们不雇佣初级工程师的原因是，让高级工程师花时间给他们提供指导，对我们来说成本太高了。」我已经了解高级工程师的价格，因为我就是其中之一，并且为了预估项目预算，项目经理曾让我给项目分配时间。我知道的价格区间是 190 ~ 300 美元每小时。这就是很多公司认为雇佣初级工程师是一笔损失的原因。我并不这么认为：没有高级工程师能够一直高效工作一整天。公司对人力成本的焦虑就像鳄鱼的眼泪，（至少以我的观点来说）他们刻意不去思考浪费在很多事物上的时间，比如开会。但让我们来做个假设，他们将初级开发者的职位重新加入到团队。另一个问题出现了：高级工程师根本没有与初级工程师合作或者培训他人的经验。当我第一次开始与初级工程师合作时，我不知道该如何去做。我感到迷茫和困惑。我所待的公司基本上就是这样的态度：“让他们有事可做，让他们可以从中学到东西。”但是，这样做真的不可持续。我寻找资源，但是并没有找到。如果你知道任何资源，请在留言中通知我。我最终拼凑了各种课程和不同作业。但令人惊叹的是，我在做这件事时学到了很多东西。直到我必须解释 Javascript 语言的特性，我才觉得我真的深入地理解了它们。我为教学开发的一些工具最终付诸于项目。现在，有一些时候令我感到沮丧。特别是当项目经理或其他经理不了解现实状况的时候。他们总觉得，这些人教了就马上能够进行开发，但这之间有个消化和理解的过程。我认为我想说的是：整个软件开发生态系统需要初级工程师以保持健康。培训他们有成本，但也有好处。我建议那些想要再次招聘初级工程师的公司，投入一些时间用来制定一个大纲，用来帮助高级工程师以及任何与他们合作的人员有效地辅导。并且说明下这个严峻的现实。就像并不是所有初级工程师能够成为成功的开发者。那样的话，你会做什么呢？抱怨辅导你的高级工程师？或者追逐那些奋斗于通往成功领域（如项目管理、销售工程师或者其他非开发的角色）的人。在这些领域，软件技能也是非常重要的。并且并不是所有的高级工程师能够成为成功的导师。很多杰出的工程师不具备这一特质。他们应该避免扮演这样的角色。对于那些必须担任导师这一角色的人，如果他们没做好，我们也不应该苛责他们。我曾在一个团队中给初级开发者提供大部分的指导。与其他工程师所做的工作相比，这被认为不是“真正”的工作，这后来也让我不太愿意担当这个角色。是的，我会将性别考虑进去，因为我是一位女性，并且当女性担任类似这种角色，受刻板印象的影响，她们总被认为是“训导员”。那意味着更低的声誉，更低的声誉意味着更少的工资。话虽这么说，但如果没有提及一些其他阻碍初级工程师的经济问题，我不足以写下这篇文章。最近，因为一个活动，我拜访了一家公司，他们大概的意思就是说，现在所有“容易”的工作都已外包给另一个国家。这些工作以前都是初级工程师做的。之后有了自动化。我还是初级工程师时许多需要亲自做的工作，现在都可以自动化处理了。对于那些初级工程师，找到你的第一份工作正变得越来越困难。你可能不得不做一些我不愿意推荐的事，比如免费给各种项目打工。如果你确实选择了一个非常好的开源项目，你可以将它写到简历上。我不太倾向于推荐为“创业公司”免费打工。你也要寻找你自己的导师。现场见面会是最好的方式，虽然我明白并不是每个人都喜欢这样，因此你可以试试 Slack 和 Discord 聊天应用。不过就像很多约会一样，这也会变得糟糕。你将被多次的拒绝。你将做一些糟糕的、甚至完全失败的项目，因为和商业项目的人员相比，免费项目的工作人员一般有点更古里古怪。就像一个初级工程师告诉我的：他们不再去某个见面会，因为他们之前做的项目彻底地失败了。我不得不告诉他们应该继续寻找项目，但心中要明白大多数项目都不是完善的。对我而言，我很高兴为参加见面会的人提供辅导。在这些背景下，我也要努力地制定一份更正式的导师计划。我不确定整个行业的解决方案是什么。我不确定缺乏初级工程师的公司是不平衡的还是聪明的。实际情况是，大多数软件开发人员不会长时间呆在一个地方，所以也许投入大量资源来培训人员是没有意义的。或者说，这个行业也许应该问问自己，为什么人们不停地跳槽？也许是因为大多数公司都很糟糕，或者对我们很多人来说，这是提高薪水的唯一途径。我可以等待一个愚蠢的、毫无意义的年度“绩效评估”让我涨 1％ 的工资。或者投递简历，通过面试，拿到 10％ 或更多的工资涨幅。这不仅仅是个别公司不够完善的信号，也是整个行业不够完善的信号。", "author": "伯乐在线", "tags": ["职场", "程序员", "职场"]}
{"url_object_id": "1b36123333a36dfc38deb859d6cb3ce6", "title": "分布式之消息队列复习精讲", "url": "http://blog.jobbole.com/114016/", "create_date": "2018/05/21", "praise_nums": "2", "favor_nums": 3, "comment_nums": 0, "content": "为什么写这篇文章?博主有两位朋友分别是小A和小B:庆幸的是两位朋友都很有上进心，于是博主写这篇文章，帮助他们复习一下关于消息队列中间件这块的要点复习要点本文大概围绕如下几点进行阐述:我们围绕以上七点进行阐述。需要说明一下，本文不是《消息队列从入门到精通》这种课程，因此只是提供一个复习思路，而不是去教你们怎么调用消息队列的API。建议对消息队列不了解的人，去找点消息队列的博客看看，再看本文，收获更大1、为什么要使用消息队列?分析:一个用消息队列的人，不知道为啥用，这就有点尴尬。没有复习这点，很容易被问蒙，然后就开始胡扯了。\n回答:这个问题,咱只答三个最主要的应用场景(不可否认还有其他的，但是只答三个主要的),即以下六个字:解耦、异步、削峰传统模式:\n\n传统模式的缺点：系统间耦合性太强，如上图所示，系统A在代码中直接调用系统B和系统C的代码，如果将来D系统接入，系统A还需要修改代码，过于麻烦！中间件模式:\n\n中间件模式的的优点：将消息写入消息队列，需要消息的系统自己从消息队列中订阅，从而系统A不需要做任何修改。传统模式:\n\n传统模式的缺点：一些非必要的业务逻辑以同步的方式运行，太耗费时间。中间件模式:\n\n中间件模式的的优点：将消息写入消息队列，非必要的业务逻辑以异步的方式运行，加快响应速度传统模式\n\n传统模式的缺点：并发量大的时候，所有的请求直接怼到数据库，造成数据库连接异常中间件模式:\n\n中间件模式的的优点：系统A慢慢的按照数据库能处理的并发量，从消息队列中慢慢拉取消息。在生产中，这个短暂的高峰期积压是允许的。2、使用了消息队列会有什么缺点?分析:一个使用了MQ的项目，如果连这个问题都没有考虑过，就把MQ引进去了，那就给自己的项目带来了风险。我们引入一个技术，要对这个技术的弊端有充分的认识，才能做好预防。要记住，不要给公司挖坑！\n回答:回答也很容易，从以下两个个角度来答:你想啊，本来其他系统只要运行好好的，那你的系统就是正常的。现在你非要加个消息队列进去，那消息队列挂了，你的系统不是呵呵了。因此，系统可用性降低:要多考虑很多方面的问题，比如一致性问题、如何保证消息不被重复消费，如何保证保证消息可靠传输。因此，需要考虑的东西更多，系统复杂性增大。但是，我们该用还是要用的。3、消息队列如何选型?先说一下，博主只会ActiveMQ,RabbitMQ,RocketMQ,Kafka，对什么ZeroMQ等其他MQ没啥理解，因此只能基于这四种MQ给出回答。\n分析:既然在项目中用了MQ，肯定事先要对业界流行的MQ进行调研，如果连每种MQ的优缺点都没了解清楚，就拍脑袋依据喜好，用了某种MQ，还是给项目挖坑。如果面试官问:”你为什么用这种MQ？。”你直接回答”领导决定的。”这种回答就很LOW了。还是那句话，不要给公司挖坑。\n回答:首先，咱先上ActiveMQ的社区，看看该MQ的更新频率:Apache ActiveMQ 5.15.3 ReleaseApache ActiveMQ 5.15.2 ReleasedApache ActiveMQ 5.15.0 Released我们可以看出，ActiveMq几个月才发一次版本，据说研究重心在他们的下一代产品Apollo。\n接下来，我们再去RabbitMQ的社区去看一下,RabbitMQ的更新频率RabbitMQ 3.7.3 release  30 January 2018RabbitMQ 3.7.2 release23 December 2017我们可以看出，RabbitMQ版本发布比ActiveMq频繁很多。至于RocketMQ和kafka就不带大家看了，总之也比ActiveMQ活跃的多。详情，可自行查阅。\n再来一个性能对比表综合上面的材料得出以下两点:\n(1)中小型软件公司，建议选RabbitMQ.一方面，erlang语言天生具备高并发的特性，而且他的管理界面用起来十分方便。正所谓，成也萧何，败也萧何！他的弊端也在这里，虽然RabbitMQ是开源的，然而国内有几个能定制化开发erlang的程序员呢？所幸，RabbitMQ的社区十分活跃，可以解决开发过程中遇到的bug，这点对于中小型公司来说十分重要。不考虑rocketmq和kafka的原因是，一方面中小型软件公司不如互联网公司，数据量没那么大，选消息中间件，应首选功能比较完备的，所以kafka排除。不考虑rocketmq的原因是，rocketmq是阿里出品，如果阿里放弃维护rocketmq，中小型公司一般抽不出人来进行rocketmq的定制化开发，因此不推荐。\n(2)大型软件公司，根据具体使用在rocketMq和kafka之间二选一。一方面，大型软件公司，具备足够的资金搭建分布式环境，也具备足够大的数据量。针对rocketMQ,大型软件公司也可以抽出人手对rocketMQ进行定制化开发，毕竟国内有能力改JAVA源码的人，还是相当多的。至于kafka，根据业务场景选择，如果有日志采集功能，肯定是首选kafka了。具体该选哪个，看使用场景。4、如何保证消息队列是高可用的？分析:在第二点说过了，引入消息队列后，系统的可用性下降。在生产中，没人使用单机模式的消息队列。因此，作为一个合格的程序员，应该对消息队列的高可用有很深刻的了解。如果面试的时候，面试官问，你们的消息中间件如何保证高可用的？你的回答只是表明自己只会订阅和发布消息，面试官就会怀疑你是不是只是自己搭着玩，压根没在生产用过。请做一个爱思考，会思考，懂思考的程序员。\n回答:这问题，其实要对消息队列的集群模式要有深刻了解，才好回答。\n以rcoketMQ为例，他的集群就有多master 模式、多master多slave异步复制模式、多 master多slave同步双写模式。多master多slave模式部署架构图(网上找的,偷个懒，懒得画):\n\n其实博主第一眼看到这个图，就觉得和kafka好像，只是NameServer集群，在kafka中是用zookeeper代替，都是用来保存和发现master和slave用的。通信过程如下:\nProducer 与 NameServer集群中的其中一个节点（随机选择）建立长连接，定期从 NameServer 获取 Topic 路由信息，并向提供 Topic 服务的 Broker Master 建立长连接，且定时向 Broker 发送心跳。Producer 只能将消息发送到 Broker master，但是 Consumer 则不一样，它同时和提供 Topic 服务的 Master 和 Slave建立长连接，既可以从 Broker Master 订阅消息，也可以从 Broker Slave 订阅消息。\n那么kafka呢,为了对比说明直接上kafka的拓补架构图(也是找的，懒得画)\n\n如上图所示，一个典型的Kafka集群中包含若干Producer（可以是web前端产生的Page View，或者是服务器日志，系统CPU、Memory等），若干broker（Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高），若干Consumer Group，以及一个Zookeeper集群。Kafka通过Zookeeper管理集群配置，选举leader，以及在Consumer Group发生变化时进行rebalance。Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。\n至于rabbitMQ,也有普通集群和镜像集群模式，自行去了解，比较简单，两小时即懂。\n要求，在回答高可用的问题时，应该能逻辑清晰的画出自己的MQ集群架构或清晰的叙述出来。5、如何保证消息不被重复消费？分析:这个问题其实换一种问法就是，如何保证消息队列的幂等性?这个问题可以认为是消息队列领域的基本问题。换句话来说，是在考察你的设计能力，这个问题的回答可以根据具体的业务场景来答，没有固定的答案。\n回答:先来说一下为什么会造成重复消费?\n其实无论是那种消息队列，造成重复消费原因其实都是类似的。正常情况下，消费者在消费消息时候，消费完毕后，会发送一个确认信息给消息队列，消息队列就知道该消息被消费了，就会将该消息从消息队列中删除。只是不同的消息队列发送的确认信息形式不同,例如RabbitMQ是发送一个ACK确认消息，RocketMQ是返回一个CONSUME_SUCCESS成功标志，kafka实际上有个offset的概念，简单说一下(如果还不懂，出门找一个kafka入门到精通教程),就是每一个消息都有一个offset，kafka消费过消息后，需要提交offset，让消息队列知道自己已经消费过了。那造成重复消费的原因?，就是因为网络传输等等故障，确认信息没有传送到消息队列，导致消息队列不知道自己已经消费过该消息了，再次将该消息分发给其他的消费者。\n如何解决?这个问题针对业务场景来答分以下几点\n(1)比如，你拿到这个消息做数据库的insert操作。那就容易了，给这个消息做一个唯一主键，那么就算出现重复消费的情况，就会导致主键冲突，避免数据库出现脏数据。\n(2)再比如，你拿到这个消息做redis的set的操作，那就容易了，不用解决，因为你无论set几次结果都是一样的，set操作本来就算幂等操作。\n(3)如果上面两种情况还不行，上大招。准备一个第三方介质,来做消费记录。以redis为例，给消息分配一个全局id，只要消费过该消息，将<id,message>以K-V形式写入redis。那消费者开始消费前，先去redis中查询有没消费记录即可。6、如何保证消费的可靠性传输?分析:我们在使用消息队列的过程中，应该做到消息不能多消费，也不能少消费。如果无法做到可靠性传输，可能给公司带来千万级别的财产损失。同样的，如果可靠性传输在使用过程中，没有考虑到，这不是给公司挖坑么，你可以拍拍屁股走了，公司损失的钱，谁承担。还是那句话，认真对待每一个项目，不要给公司挖坑。\n回答:其实这个可靠性传输，每种MQ都要从三个角度来分析:生产者弄丢数据、消息队列弄丢数据、消费者弄丢数据(1)生产者丢数据\n从生产者弄丢数据这个角度来看，RabbitMQ提供transaction和confirm模式来确保生产者不丢消息。\ntransaction机制就是说，发送消息前，开启事物(channel.txSelect())，然后发送消息，如果发送过程中出现什么异常，事物就会回滚(channel.txRollback())，如果发送成功则提交事物(channel.txCommit())。\n然而缺点就是吞吐量下降了。因此，按照博主的经验，生产上用confirm模式的居多。一旦channel进入confirm模式，所有在该信道上面发布的消息都将会被指派一个唯一的ID(从1开始)，一旦消息被投递到所有匹配的队列之后，rabbitMQ就会发送一个Ack给生产者(包含消息的唯一ID)，这就使得生产者知道消息已经正确到达目的队列了.如果rabiitMQ没能处理该消息，则会发送一个Nack消息给你，你可以进行重试操作。处理Ack和Nack的代码如下所示（说好不上代码的，偷偷上了）:channel.addConfirmListener(new ConfirmListener() {                  public void handleNack(long deliveryTag, boolean multiple) throws IOException {                  }                  public void handleAck(long deliveryTag, boolean multiple) throws IOException {                  }  (2)消息队列丢数据\n处理消息队列丢数据的情况，一般是开启持久化磁盘的配置。这个持久化配置可以和confirm机制配合使用，你可以在消息持久化磁盘后，再给生产者发送一个Ack信号。这样，如果消息持久化磁盘之前，rabbitMQ阵亡了，那么生产者收不到Ack信号，生产者会自动重发。\n那么如何持久化呢，这里顺便说一下吧，其实也很容易，就下面两步\n1、将queue的持久化标识durable设置为true,则代表是一个持久的队列\n2、发送消息的时候将deliveryMode=2\n这样设置以后，rabbitMQ就算挂了，重启后也能恢复数据\n(3)消费者丢数据\n消费者丢数据一般是因为采用了自动确认消息模式。这种模式下，消费者会自动确认收到信息。这时rahbitMQ会立即将消息删除，这种情况下如果消费者出现异常而没能处理该消息，就会丢失该消息。\n至于解决方案，采用手动确认消息即可。这里先引一张kafka Replication的数据流向图\n\nProducer在发布消息到某个Partition时，先通过ZooKeeper找到该Partition的Leader，然后无论该Topic的Replication Factor为多少（也即该Partition有多少个Replica），Producer只将该消息发送到该Partition的Leader。Leader会将该消息写入其本地Log。每个Follower都从Leader中pull数据。\n针对上述情况，得出如下分析\n(1)生产者丢数据\n在kafka生产中，基本都有一个leader和多个follwer。follwer会去同步leader的信息。因此，为了避免生产者丢数据，做如下两点配置(2)消息队列丢数据\n针对消息队列丢数据的情况，无外乎就是，数据还没同步，leader就挂了，这时zookpeer会将其他的follwer切换为leader,那数据就丢失了。针对这种情况，应该做两个配置。这两个配置加上上面生产者的配置联合起来用，基本可确保kafka不丢数据(3)消费者丢数据\n这种情况一般是自动提交了offset，然后你处理程序过程中挂了。kafka以为你处理好了。再强调一次offset是干嘛的\noffset：指的是kafka的topic中的每个消费组消费的下标。简单的来说就是一条消息对应一个offset下标，每次消费数据的时候如果提交offset，那么下次消费就会从提交的offset加一那里开始消费。\n比如一个topic中有100条数据，我消费了50条并且提交了，那么此时的kafka服务端记录提交的offset就是49(offset从0开始)，那么下次消费的时候offset就从50开始消费。\n解决方案也很简单，改成手动提交即可。大家自行查阅吧7、如何保证消息的顺序性？分析:其实并非所有的公司都有这种业务需求，但是还是对这个问题要有所复习。\n回答:针对这个问题，通过某种算法，将需要保持先后顺序的消息放到同一个消息队列中(kafka中就是partition,rabbitMq中就是queue)。然后只用一个消费者去消费该队列。\n有的人会问:那如果为了吞吐量，有多个消费者去消费怎么办？\n这个问题，没有固定回答的套路。比如我们有一个微博的操作，发微博、写评论、删除微博，这三个异步操作。如果是这样一个业务场景，那只要重试就行。比如你一个消费者先执行了写评论的操作，但是这时候，微博都还没发，写评论一定是失败的，等一段时间。等另一个消费者，先执行写评论的操作后，再执行，就可以成功。\n总之，针对这个问题，我的观点是保证入队有序就行，出队以后的顺序交给消费者自己去保证，没有固定套路。写到这里，希望读者把本文提出的这几个问题，经过深刻的准备后，一般来说，能囊括大部分的消息队列的知识点。如果面试官不问这几个问题怎么办，简单，自己把几个问题讲清楚，突出以下自己考虑的全面性。\n最后，其实我不太提倡这样突击复习，希望大家打好基本功，做一个爱思考，懂思考，会思考的程序员。", "author": "孤独烟", "tags": ["IT技术", "分布式", "数据库"]}
{"url_object_id": "0f3f2c33552a25853e5c09c5da5cf3a9", "title": "如何编译 Linux 内核", "url": "http://blog.jobbole.com/114047/", "create_date": "2018/05/25", "praise_nums": "1", "favor_nums": 1, "comment_nums": 0, "content": "曾经有一段时间，升级 Linux 内核让很多用户打心里有所畏惧。在那个时候，升级内核包含了很多步骤，也需要很多时间。现在，内核的安装可以轻易地通过像 apt 这样的包管理器来处理。通过添加特定的仓库，你能很轻易地安装实验版本的或者指定版本的内核（比如针对音频产品的实时内核）。考虑一下，既然升级内核如此容易，为什么你不愿意自行编译一个呢？这里列举一些可能的原因：你想要简单了解编译内核的过程你需要启用或者禁用内核中特定的选项，因为它们没有出现在标准选项里你想要启用标准内核中可能没有添加的硬件支持你使用的发行版需要你编译内核你是一个学生，而编译内核是你的任务不管出于什么原因，懂得如何编译内核是非常有用的，而且可以被视作一个通行权。当我第一次编译一个新的 Linux 内核（那是很久以前了），然后尝试从它启动，我从中（系统马上就崩溃了，然后不断地尝试和失败）感受到一种特定的兴奋。既然这样，让我们来实验一下编译内核的过程。我将使用 Ubuntu 16.04 Server 来进行演示。在运行了一次常规的 sudo apt upgrade 之后，当前安装的内核版本是 4.4.0-121。我想要升级内核版本到 4.17， 让我们小心地开始吧。有一个警告：强烈建议你在虚拟机里实验这个过程。基于虚拟机，你总能创建一个快照，然后轻松地从任何问题中回退出来。不要在产品机器上使用这种方式升级内核，除非你知道你在做什么。下载内核我们要做的第一件事是下载内核源码。在 Kernel.org 找到你要下载的所需内核的 URL。找到 URL 之后，使用如下命令（我以 4.17 RC2 内核为例） 来下载源码文件:wget https://git.kernel.org/torvalds/t/linux-4.17-rc2.tar.gz在下载期间，有一些事需要去考虑。安装需要的环境为了编译内核，我们首先得安装一些需要的环境。这可以通过一个命令来完成：sudo apt-get install git fakeroot build-essential ncurses-dev xz-utils libssl-dev bc flex libelf-dev bison务必注意：你将需要至少 128GB 的本地可用磁盘空间来完成内核的编译过程。因此你必须确保有足够的空间。解压源码在新下载的内核所在的文件夹下，使用该命令来解压内核：tar xvzf linux-4.17-rc2.tar.gz使用命令 cd linux-4.17-rc2 进入新生成的文件夹。配置内核在正式编译内核之前，我们首先必须配置需要包含哪些模块。实际上，有一些非常简单的方式来配置。使用一个命令，你能拷贝当前内核的配置文件，然后使用可靠的 menuconfig 命令来做任何必要的更改。使用如下命令来完成：cp /boot/config-$(uname -r) .config现在你有一个配置文件了，输入命令 make menuconfig。该命令将打开一个配置工具（图 1），它可以让你遍历每个可用模块，然后启用或者禁用你需要或者不需要的模块。很有可能你会禁用掉内核中的一个重要部分，所以在 menuconfig 期间小心地一步步进行。如果你对某个选项不确定，不要去管它。或者更好的方法是使用我们拷贝的当前运行的内核的配置文件（因为我们知道它可以工作）。一旦你已经遍历了整个配置列表（它非常长），你就准备好开始编译了。编译和安装现在是时候去实际地编译内核了。第一步是使用 make 命令去编译。调用 make 命令然后回答必要的问题（图 2）。这些问题取决于你将升级的现有内核以及升级后的内核。相信我，将会有非常多的问题要回答，因此你得预留大量的时间。回答了长篇累牍的问题之后，你就可以用如下的命令安装那些之前启用的模块：make modules_又来了，这个命令将耗费一些时间，所以要么坐下来看着编译输出，或者去做些其他事（因为编译期间不需要你的输入）。可能的情况是，你想要去进行别的任务（除非你真的喜欢看着终端界面上飞舞而过的输出）。现在我们使用这个命令来安装内核：sudo make install又一次，另一个将要耗费大量可观时间的命令。事实上，make install 命令将比 make modules_install 命令花费更多的时间。去享用午餐，配置一个路由器，将 Linux 安装在一些服务器上，或者小睡一会吧。启用内核作为引导一旦 make install 命令完成了，就是时候将内核启用来作为引导。使用这个命令来实现：sudo update-initramfs -c -k 4.17-rc2当然，你需要将上述内核版本号替换成你编译完的。当命令执行完毕后，使用如下命令来更新 grub：sudo update-grub现在你可以重启系统并且选择新安装的内核了。恭喜!你已经编译了一个 Linux 内核！它是一项耗费时间的活动；但是，最终你的 Linux 发行版将拥有一个定制的内核，同时你也将拥有一项被许多 Linux 管理员所倾向忽视的重要技能。从 Linux 基金会和 edX 提供的免费 “Introduction to Linux” 课程来学习更多的 Linux 知识。", "author": "Jack Wallen", "tags": ["IT技术", "Linux"]}
{"url_object_id": "4382c32a2cdca2e189d33b686dc851bb", "title": "详解 Linux 文档属性、拥有者、群组、权限、差异", "url": "http://blog.jobbole.com/114031/", "create_date": "2018/05/22", "praise_nums": "1", "favor_nums": 3, "comment_nums": 0, "content": "我们都知道Linux是一个支持多用户、多任务的系统，这也是它最优秀的特性，即可能同时有很多人都在系统上进行工作，所以千万不要强制关机，同时，为了保护每个人的隐私和工作环境，针对某一个文档(文件、目录)，Linux系统定义了三种身份，分别是拥有者(owner)、群组(group)、其他人(others)，每一种身份又对应三种权限，分别是可读(readable)、可写(writable)、可执行(excutable)。使用命令ls -al --full-time，或者此命令的简写ll可以查看文件或者目录的所有属性。如下：\n\n从上面可以看到，每一行都有7列，分别是：位置etc/passwd语法chown [-R] [] []-R 递归变更，即连同次目录下的所有文件(夹)都要变更。用法chown daemon test 变更文件夹test账号为daemon。\n\nchown daemon:root test 变更文件夹test群组为root。\nchown root.users test 变更文件夹账号为root，群组为users\n\nchown .root test 单独变更群组为root\n位置etc/group语法chgrp [-options] [] []用法chgrp -R users test 改变test文件夹及其所有子文件(夹)的群组为users。\nLinux文档的基本权限就三个，分别是read/write/execute，加上身份owner/group/others也只有九个。权限变更的方式有2种，分别是符号法和数字法。符号法分别使用u，g，o来代表三种身份，a表示全部身份；分别使用r、w、x表示三种权限；分别使用+、-、=表示操作行为chmod | u g o a | + - = | r w x |  设置权限(=)变更目录test的权限为任何人都可读、写、执行。chmod u=rwx,g=rwx,o=rwx test chmod ugo=rwx test chmod a=rwx test去掉权限(-)去掉目录test执行权限chmod u-x,g-x,o-x test chmod ugo-x test chmod a-x test添加权限(+)增加目录test执行权限chmod u+x,g+x,o+x test chmod ugo+x test chmod a+x test数字法顾名思义，就是使用数字来代表权限，r,w,x分别为4,2,1。三种权限累加就可以得出一种身份的权限。设置目录test的权限为任何人都可读、写、执行。chmod 777 test \n设置目录test的权限为任何人都可读、写。chmod 666 test 赋予一个shell文件test.sh可执行权限，拥有者可读、写、执行，群组账号和其他人可读、执行。chmod 755 test 文档权限对于文件和目录有巨大的差异文件针对的是该文件内容readable 可读取该文件的实际内容writable 可以编辑、新增或者是修改该文件的内容executable 有可以被系统执行的权限目录针对的是该目录下的文件对象readable 具有读取目录结构清单的权限，即可以通过命令，查询该目录清单。writable 具有变动该目录结构清单的权限，即可以创建、迁移、删除、更名该目录下的文件。executable 具备进入该目录的权限，即可以通过命令，转到工作目录。Linux的每个文档可以分别针对三种身份赋予rwx权限；chgrp命令变更文件群组，chmod命令变更文件权限，chown变更文件拥有者；那么以后记得使用文档权限来保护数据的安全性哦。如果你觉得本篇文章对您有帮助的话，感谢您的【推荐】。\n如果你对 linux 感兴趣的话可以关注我，我会定期的在博客分享我的学习心得。", "author": "无痴迷，不成功", "tags": ["IT技术", "Linux"]}
{"url_object_id": "821f7f4efb272c0b6522e2a1e27276be", "title": "我必须得告诉大家的MySQL优化原理", "url": "http://blog.jobbole.com/114041/", "create_date": "2018/05/25", "praise_nums": "1", "favor_nums": 3, "comment_nums": 0, "content": "说起MySQL的查询优化，相信大家收藏了一堆奇技淫巧：不能使用SELECT *、不使用NULL字段、合理创建索引、为字段选择合适的数据类型….. 你是否真的理解这些优化技巧？是否理解其背后的工作原理？在实际场景下性能真有提升吗？我想未必。因而理解这些优化建议背后的原理就尤为重要，希望本文能让你重新审视这些优化建议，并在实际业务场景下合理的运用。MySQL逻辑架构如果能在头脑中构建一幅MySQL各组件之间如何协同工作的架构图，有助于深入理解MySQL服务器。下图展示了MySQL的逻辑架构图。MySQL逻辑架构整体分为三层，最上层为客户端层，并非MySQL所独有，诸如：连接处理、授权认证、安全等功能均在这一层处理。MySQL大多数核心服务均在中间这一层，包括查询解析、分析、优化、缓存、内置函数(比如：时间、数学、加密等函数)。所有的跨存储引擎的功能也在这一层实现：存储过程、触发器、视图等。最下层为存储引擎，其负责MySQL中的数据存储和提取。和Linux下的文件系统类似，每种存储引擎都有其优势和劣势。中间的服务层通过API与存储引擎通信，这些API接口屏蔽了不同存储引擎间的差异。MySQL查询过程我们总是希望MySQL能够获得更高的查询性能，最好的办法是弄清楚MySQL是如何优化和执行查询的。一旦理解了这一点，就会发现：很多的查询优化工作实际上就是遵循一些原则让MySQL的优化器能够按照预想的合理方式运行而已。当向MySQL发送一个请求的时候，MySQL到底做了些什么呢？MySQL客户端/服务端通信协议是“半双工”的：在任一时刻，要么是服务器向客户端发送数据，要么是客户端向服务器发送数据，这两个动作不能同时发生。一旦一端开始发送消息，另一端要接收完整个消息才能响应它，所以我们无法也无须将一个消息切成小块独立发送，也没有办法进行流量控制。客户端用一个单独的数据包将查询请求发送给服务器，所以当查询语句很长的时候，需要设置max_allowed_packet参数。但是需要注意的是，如果查询实在是太大，服务端会拒绝接收更多数据并抛出异常。与之相反的是，服务器响应给用户的数据通常会很多，由多个数据包组成。但是当服务器响应客户端请求时，客户端必须完整的接收整个返回结果，而不能简单的只取前面几条结果，然后让服务器停止发送。因而在实际开发中，尽量保持查询简单且只返回必需的数据，减小通信间数据包的大小和数量是一个非常好的习惯，这也是查询中尽量避免使用SELECT *以及加上LIMIT限制的原因之一。在解析一个查询语句前，如果查询缓存是打开的，那么MySQL会检查这个查询语句是否命中查询缓存中的数据。如果当前查询恰好命中查询缓存，在检查一次用户权限后直接返回缓存中的结果。这种情况下，查询不会被解析，也不会生成执行计划，更不会执行。MySQL将缓存存放在一个引用表（不要理解成table，可以认为是类似于HashMap的数据结构），通过一个哈希值索引，这个哈希值通过查询本身、当前要查询的数据库、客户端协议版本号等一些可能影响结果的信息计算得来。所以两个查询在任何字符上的不同（例如：空格、注释），都会导致缓存不会命中。如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、mysql库中的系统表，其查询结果\n都不会被缓存。比如函数NOW()或者CURRENT_DATE()会因为不同的查询时间，返回不同的查询结果，再比如包含CURRENT_USER或者CONNECION_ID()的查询语句会因为不同的用户而返回不同的结果，将这样的查询结果缓存起来没有任何的意义。既然是缓存，就会失效，那查询缓存何时失效呢？MySQL的查询缓存系统会跟踪查询中涉及的每个表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效。正因为如此，在任何的写操作时，MySQL必须将对应表的所有缓存都设置为失效。如果查询缓存非常大或者碎片很多，这个操作就可能带来很大的系统消耗，甚至导致系统僵死一会儿。而且查询缓存对系统的额外消耗也不仅仅在写操作，读操作也不例外：基于此，我们要知道并不是什么情况下查询缓存都会提高系统性能，缓存和失效都会带来额外消耗，只有当缓存带来的资源节约大于其本身消耗的资源时，才会给系统带来性能提升。但要如何评估打开缓存是否能够带来性能提升是一件非常困难的事情，也不在本文讨论的范畴内。如果系统确实存在一些性能问题，可以尝试打开查询缓存，并在数据库设计上做一些优化，比如：最后的忠告是不要轻易打开查询缓存，特别是写密集型应用。如果你实在是忍不住，可以将query_cache_type设置为DEMAND，这时只有加入SQL_CACHE的查询才会走缓存，其他查询则不会，这样可以非常自由地控制哪些查询需要被缓存。当然查询缓存系统本身是非常复杂的，这里讨论的也只是很小的一部分，其他更深入的话题，比如：缓存是如何使用内存的？如何控制内存的碎片化？事务对查询缓存有何影响等等，读者可以自行阅读相关资料，这里权当抛砖引玉吧。MySQL通过关键字将SQL语句进行解析，并生成一颗对应的解析树。这个过程解析器主要通过语法规则来验证和解析。比如SQL中是否使用了错误的关键字或者关键字的顺序是否正确等等。预处理则会根据MySQL规则进一步检查解析树是否合法。比如检查要查询的数据表和数据列是否存在等等。经过前面的步骤生成的语法树被认为是合法的了，并且由优化器将其转化成查询计划。多数情况下，一条查询可以有很多种执行方式，最后都返回相应的结果。优化器的作用就是找到这其中最好的执行计划。MySQL使用基于成本的优化器，它尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最小的一个。在MySQL可以通过查询当前会话的last_query_cost的值来得到其计算当前查询的成本。mysql> select * from t_message limit 10;+-----------------+-------------++-----------------+-------------++-----------------+-------------+示例中的结果表示优化器认为大概需要做6391个数据页的随机查找才能完成上面的查询。这个结果是根据一些列的统计信息计算得来的，这些统计信息包括：每张表或者索引的页面个数、索引的基数、索引和数据行的长度、索引的分布情况等等。有非常多的原因会导致MySQL选择错误的执行计划，比如统计信息不准确、不会考虑不受其控制的操作成本（用户自定义函数、存储过程）、MySQL认为的最优跟我们想的不一样（我们希望执行时间尽可能短，但MySQL值选择它认为成本小的，但成本小并不意味着执行时间短）等等。MySQL的查询优化器是一个非常复杂的部件，它使用了非常多的优化策略来生成一个最优的执行计划：重新定义表的关联顺序（多张表关联查询时，并不一定按照SQL中指定的顺序进行，但有一些技巧可以指定关联顺序）优化和函数（找某列的最小值，如果该列有索引，只需要查找B+Tree索引最左端，反之则可以找到最大值，具体原理见下文）提前终止查询（比如：使用Limit时，查找到满足数量的结果集后会立即终止查询）优化排序（在老版本MySQL会使用两次传输排序，即先读取行指针和需要排序的字段在内存中对其排序，然后再根据排序结果去读取数据行，而新版本采用的是单次传输排序，也就是一次读取所有的数据行，然后根据给定的列排序。对于I/O密集型应用，效率会高很多）随着MySQL的不断发展，优化器使用的优化策略也在不断的进化，这里仅仅介绍几个非常常用且容易理解的优化策略，其他的优化策略，大家自行查阅吧。在完成解析和优化阶段以后，MySQL会生成对应的执行计划，查询执行引擎根据执行计划给出的指令逐步执行得出结果。整个执行过程的大部分操作均是通过调用存储引擎实现的接口来完成，这些接口被称为handler API。查询过程中的每一张表由一个handler实例表示。实际上，MySQL在查询优化阶段就为每一张表创建了一个handler实例，优化器可以根据这些实例的接口来获取表的相关信息，包括表的所有列名、索引统计信息等。存储引擎接口提供了非常丰富的功能，但其底层仅有几十个接口，这些接口像搭积木一样完成了一次查询的大部分操作。查询执行的最后一个阶段就是将结果返回给客户端。即使查询不到数据，MySQL仍然会返回这个查询的相关信息，比如该查询影响到的行数以及执行时间等等。如果查询缓存被打开且这个查询可以被缓存，MySQL也会将结果存放到缓存中。结果集返回客户端是一个增量且逐步返回的过程。有可能MySQL在生成第一条结果时，就开始向客户端逐步返回结果集了。这样服务端就无须存储太多结果而消耗过多内存，也可以让客户端第一时间获得返回结果。需要注意的是，结果集中的每一行都会以一个满足①中所描述的通信协议的数据包发送，再通过TCP协议进行传输，在传输过程中，可能对MySQL的数据包进行缓存然后批量发送。回头总结一下MySQL整个查询执行过程，总的来说分为6个步骤：性能优化建议看了这么多，你可能会期待给出一些优化手段，是的，下面会从3个不同方面给出一些优化建议。但请等等，还有一句忠告要先送给你：不要听信你看到的关于优化的“绝对真理”，包括本文所讨论的内容，而应该是在实际的业务场景下通过测试来验证你关于执行计划以及响应时间的假设。选择数据类型只要遵循小而简单的原则就好，越小的数据类型通常会更快，占用更少的磁盘、内存，处理时需要的CPU周期也更少。越简单的数据类型在计算时需要更少的CPU周期，比如，整型就比字符操作代价低，因而会使用整型来存储ip地址，使用DATETIME来存储时间，而不是使用字符串。这里总结几个可能容易理解错误的技巧：索引是提高MySQL查询性能的一个重要途径，但过多的索引可能会导致过高的磁盘使用率以及过高的内存占用，从而影响应用程序的整体性能。应当尽量避免事后才想起添加索引，因为事后可能需要监控大量的SQL才能定位到问题所在，而且添加索引的时间肯定是远大于初始添加索引所需要的时间，可见索引的添加也是非常有技术含量的。接下来将向你展示一系列创建高性能索引的策略，以及每条策略其背后的工作原理。但在此之前，先了解与索引相关的一些算法和数据结构，将有助于更好的理解后文的内容。通常我们所说的索引是指B-Tree索引，它是目前关系型数据库中查找数据最为常用和有效的索引，大多数存储引擎都支持这种索引。使用B-Tree这个术语，是因为MySQL在CREATE TABLE或其它语句中使用了这个关键字，但实际上不同的存储引擎可能使用不同的数据结构，比如InnoDB就是使用的B+Tree。B+Tree中的B是指balance，意为平衡。需要注意的是，B+树索引并不能找到一个给定键值的具体行，它找到的只是被查找数据行所在的页，接着数据库会把页读入到内存，再在内存中进行查找，最后得到要查找的数据。在介绍B+Tree前，先了解一下二叉查找树，它是一种经典的数据结构，其左子树的值总是小于根的值，右子树的值总是大于根的值，如下图①。如果要在这课树中查找值为5的记录，其大致流程：先找到根，其值为6，大于5，所以查找左子树，找到3，而5大于3，接着找3的右子树，总共找了3次。同样的方法，如果查找值为8的记录，也需要查找3次。所以二叉查找树的平均查找次数为(3 + 3 + 3 + 2 + 2 + 1) / 6 = 2.3次，而顺序查找的话，查找值为2的记录，仅需要1次，但查找值为8的记录则需要6次，所以顺序查找的平均查找次数为：(1 + 2 + 3 + 4 + 5 + 6) / 6 = 3.3次，因此大多数情况下二叉查找树的平均查找速度比顺序查找要快。由于二叉查找树可以任意构造，同样的值，可以构造出如图②的二叉查找树，显然这棵二叉树的查询效率和顺序查找差不多。若想二叉查找数的查询性能最高，需要这棵二叉查找树是平衡的，也即平衡二叉树（AVL树）。平衡二叉树首先需要符合二叉查找树的定义，其次必须满足任何节点的两个子树的高度差不能大于1。显然图②不满足平衡二叉树的定义，而图①是一课平衡二叉树。平衡二叉树的查找性能是比较高的（性能最好的是最优二叉树），查询性能越好，维护的成本就越大。比如图①的平衡二叉树，当用户需要插入一个新的值9的节点时，就需要做出如下变动。通过一次左旋操作就将插入后的树重新变为平衡二叉树是最简单的情况了，实际应用场景中可能需要旋转多次。至此我们可以考虑一个问题，平衡二叉树的查找效率还不错，实现也非常简单，相应的维护成本还能接受，为什么MySQL索引不直接使用平衡二叉树？随着数据库中数据的增加，索引本身大小随之增加，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级。可以想象一下一棵几百万节点的二叉树的深度是多少？如果将这么大深度的一颗二叉树放磁盘上，每读取一个节点，需要一次磁盘的I/O读取，整个查找的耗时显然是不能够接受的。那么如何减少查找过程中的I/O存取次数？一种行之有效的解决方法是减少树的深度，将二叉树变为m叉树（多路搜索树），而B+Tree就是一种多路搜索树。理解B+Tree时，只需要理解其最重要的两个特征即可：第一，所有的关键字（可以理解为数据）都存储在叶子节点（Leaf Page），非叶子节点（Index Page）并不存储真正的数据，所有记录节点都是按键值大小顺序存放在同一层叶子节点上。其次，所有的叶子节点由指针连接。如下图为高度为2的简化了的B+Tree。怎么理解这两个特征？MySQL将每个节点的大小设置为一个页的整数倍（原因下文会介绍），也就是在节点空间大小一定的情况下，每个节点可以存储更多的内结点，这样每个结点能索引的范围更大更精确。所有的叶子节点使用指针链接的好处是可以进行区间访问，比如上图中，如果查找大于20而小于30的记录，只需要找到节点20，就可以遍历指针依次找到25、30。如果没有链接指针的话，就无法进行区间查找。这也是MySQL使用B+Tree作为索引存储结构的重要原因。MySQL为何将节点大小设置为页的整数倍，这就需要理解磁盘的存储原理。磁盘本身存取就比主存慢很多，在加上机械运动损耗（特别是普通的机械硬盘），磁盘的存取速度往往是主存的几百万分之一，为了尽量减少磁盘I/O，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存，预读的长度一般为页的整数倍。MySQL巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了读取一个节点只需一次I/O。假设B+Tree的高度为h，一次检索最多需要h-1次I/O（根节点常驻内存），复杂度O(h) = O(logmN)。实际应用场景中，M通常较大，常常超过100，因此树的高度一般都比较小，通常不超过3。最后简单了解下B+Tree节点的操作，在整体上对索引的维护有一个大概的了解，虽然索引可以大大提高查询效率，但维护索引仍要花费很大的代价，因此合理的创建索引也就尤为重要。仍以上面的树为例，我们假设每个节点只能存储4个内节点。首先要插入第一个节点28，如下图所示。接着插入下一个节点70，在Index Page中查询后得知应该插入到50 – 70之间的叶子节点，但叶子节点已满，这时候就需要进行也分裂的操作，当前的叶子节点起点为50，所以根据中间值来拆分叶子节点，如下图所示。最后插入一个节点95，这时候Index Page和Leaf Page都满了，就需要做两次拆分，如下图所示。拆分后最终形成了这样一颗树。B+Tree为了保持平衡，对于新插入的值需要做大量的拆分页操作，而页的拆分需要I/O操作，为了尽可能的减少页的拆分操作，B+Tree也提供了类似于平衡二叉树的旋转功能。当Leaf Page已满但其左右兄弟节点没有满的情况下，B+Tree并不急于去做拆分操作，而是将记录移到当前所在页的兄弟节点上。通常情况下，左兄弟会被先检查用来做旋转操作。就比如上面第二个示例，当插入70的时候，并不会去做页拆分，而是左旋操作。通过旋转操作可以最大限度的减少页分裂，从而减少索引维护过程中的磁盘的I/O操作，也提高索引维护效率。需要注意的是，删除节点跟插入节点类似，仍然需要旋转和拆分操作，这里就不再说明。通过上文，相信你对B+Tree的数据结构已经有了大致的了解，但MySQL中索引是如何组织数据的存储呢？以一个简单的示例来说明，假如有如下数据表：CREATE TABLE People(    first_name varchar(50) not null,    gender enum(`m`,`f`) not null,);对于表中每一行数据，索引中包含了last_name、first_name、dob列的值，下图展示了索引是如何组织数据存储的。可以看到，索引首先根据第一个字段来排列顺序，当名字相同时，则根据第三个字段，即出生日期来排序，正是因为这个原因，才有了索引的“最左原则”。“独立的列”是指索引列不能是表达式的一部分，也不能是函数的参数。比如：select * from where id + 1 = 5我们很容易看出其等价于 id = 4，但是MySQL无法自动解析这个表达式，使用函数是同样的道理。如果列很长，通常可以索引开始的部分字符，这样可以有效节约索引空间，从而提高索引效率。在多数情况下，在多个列上建立独立的索引并不能提高查询性能。理由非常简单，MySQL不知道选择哪个索引的查询效率更好，所以在老版本，比如MySQL5.0之前就会随便选择一个列的索引，而新的版本会采用合并索引的策略。举个简单的例子，在一张电影演员表中，在actor_id和film_id两个列上都建立了独立的索引，然后有如下查询：select film_id,actor_id from film_actor where actor_id = 1 or film_id = 1老版本的MySQL会随机选择一个索引，但新版本做如下的优化：select film_id,actor_id from film_actor where actor_id = 1 select film_id,actor_id from film_actor where film_id = 1 and actor_id <> 1当出现多个索引做相交操作时（多个AND条件），通常来说一个包含所有相关列的索引要优于多个独立索引。当出现多个索引做联合操作时（多个OR条件），对结果集的合并、排序等操作需要耗费大量的CPU和内存资源，特别是当其中的某些索引的选择性不高，需要返回合并大量数据时，查询成本更高。所以这种情况下还不如走全表扫描。因此explain时如果发现有索引合并（Extra字段出现Using union），应该好好检查一下查询和表结构是不是已经是最优的，如果查询和表都没有问题，那只能说明索引建的非常糟糕，应当慎重考虑索引是否合适，有可能一个包含所有相关列的多列索引更适合。前面我们提到过索引如何组织数据存储的，从图中可以看到多列索引时，索引的顺序对于查询是至关重要的，很明显应该把选择性更高的字段放到索引的前面，这样通过第一个字段就可以过滤掉大多数不符合条件的数据。理解索引选择性的概念后，就不难确定哪个字段的选择性较高了，查一下就知道了，比如：SELECT * FROM payment where staff_id = 2 and customer_id = 584是应该创建(staff_id,customer_id)的索引还是应该颠倒一下顺序？执行下面的查询，哪个字段的选择性更接近1就把哪个字段索引前面就好。select count(distinct staff_id)/count(*) as staff_id_selectivity,       count(*) from payment多数情况下使用这个原则没有任何问题，但仍然注意你的数据中是否存在一些特殊情况。举个简单的例子，比如要查询某个用户组下有过交易的用户信息：select user_id from trade where user_group_id = 1 and trade_amount > 0MySQL为这个查询选择了索引(user_group_id,trade_amount)，如果不考虑特殊情况，这看起来没有任何问题，但实际情况是这张表的大多数数据都是从老系统中迁移过来的，由于新老系统的数据不兼容，所以就给老系统迁移过来的数据赋予了一个默认的用户组。这种情况下，通过索引扫描的行数跟全表扫描基本没什么区别，索引也就起不到任何作用。推广开来说，经验法则和推论在多数情况下是有用的，可以指导我们开发和设计，但实际情况往往会更复杂，实际业务场景下的某些特殊情况可能会摧毁你的整个设计。实际开发中，我们会经常使用多个范围条件，比如想查询某个时间段内登录过的用户：select user.* from user where login_time > '2017-04-01' and age between 18 and 30;这个查询有一个问题：它有两个范围条件，login_time列和age列，MySQL可以使用login_time列的索引或者age列的索引，但无法同时使用它们。如果一个索引包含或者说覆盖所有需要查询的字段的值，那么就没有必要再回表查询，这就称为覆盖索引。覆盖索引是非常有用的工具，可以极大的提高性能，因为查询只需要扫描索引会带来许多好处：索引条目远小于数据行大小，如果只读取索引，极大减少数据访问量索引是有按照列值顺序存储的，对于I/O密集型的范围查询要比随机从磁盘读取每一行数据的IO要少的多MySQL有两种方式可以生产有序的结果集，其一是对结果集进行排序的操作，其二是按照索引顺序扫描得出的结果自然是有序的。如果explain的结果中type列的值为index表示使用了索引扫描来做排序。扫描索引本身很快，因为只需要从一条索引记录移动到相邻的下一条记录。但如果索引本身不能覆盖所有需要查询的列，那么就不得不每扫描一条索引记录就回表查询一次对应的行。这个读取操作基本上是随机I/O，因此按照索引顺序读取数据的速度通常要比顺序地全表扫描要慢。在设计索引时，如果一个索引既能够满足排序，又满足查询，是最好的。只有当索引的列顺序和ORDER BY子句的顺序完全一致，并且所有列的排序方向也一样时，才能够使用索引来对结果做排序。如果查询需要关联多张表，则只有ORDER BY子句引用的字段全部为第一张表时，才能使用索引做排序。ORDER BY子句和查询的限制是一样的，都要满足最左前缀的要求（有一种情况例外，就是最左的列被指定为常数，下面是一个简单的示例），其他情况下都需要执行排序操作，而无法利用索引排序。// 最左列为常数，索引：(date,staff_id,customer_id)冗余索引是指在相同的列上按照相同的顺序创建的相同类型的索引，应当尽量避免这种索引，发现后立即删除。比如有一个索引(A,B)，再创建索引(A)就是冗余索引。冗余索引经常发生在为表添加新索引时，比如有人新建了索引(A,B)，但这个索引不是扩展已有的索引(A)。大多数情况下都应该尽量扩展已有的索引而不是创建新索引。但有极少情况下出现性能方面的考虑需要冗余索引，比如扩展已有索引而导致其变得过大，从而影响到其他使用该索引的查询。定期删除一些长时间未使用过的索引是一个非常好的习惯。关于索引这个话题打算就此打住，最后要说一句，索引并不总是最好的工具，只有当索引帮助提高查询速度带来的好处大于其带来的额外工作时，索引才是有效的。对于非常小的表，简单的全表扫描更高效。对于中到大型的表，索引就非常有效。对于超大型的表，建立和维护索引的代价随之增长，这时候其他技术也许更有效，比如分区表。最后的最后，后再提测是一种美德。COUNT()可能是被大家误解最多的函数了，它有两种不同的作用，其一是统计某个列值的数量，其二是统计行数。统计列值时，要求列值是非空的，它不会统计NULL。如果确认括号中的表达式不可能为空时，实际上就是在统计行数。最简单的就是当使用COUNT(*)时，并不是我们所想象的那样扩展成所有的列，实际上，它会忽略所有的列而直接统计行数。我们最常见的误解也就在这儿，在括号内指定了一列却希望统计结果是行数，而且还常常误以为前者的性能会更好。但实际并非这样，如果要统计行数，直接使用COUNT(*)，意义清晰，且性能更好。有时候某些业务场景并不需要完全精确的COUNT值，可以用近似值来代替，EXPLAIN出来的行数就是一个不错的近似值，而且执行EXPLAIN并不需要真正地去执行查询，所以成本非常低。通常来说，执行COUNT()都需要扫描大量的行才能获取到精确的数据，因此很难优化，MySQL层面还能做得也就只有覆盖索引了。如果不还能解决问题，只有从架构层面解决了，比如添加汇总表，或者使用redis这样的外部缓存系统。在大数据场景下，表与表之间通过一个冗余字段来关联，要比直接使用JOIN有更好的性能。如果确实需要使用关联查询的情况下，需要特别注意的是：确保和字句中的列上有索引。在创建索引的时候就要考虑到关联的顺序。当表A和表B用列c关联的时候，如果优化器关联的顺序是A、B，那么就不需要在A表的对应列上创建索引。没有用到的索引会带来额外的负担，一般来说，除非有其他理由，只需要在关联顺序中的第二张表的相应列上创建索引（具体原因下文分析）。确保任何的和中的表达式只涉及到一个表中的列，这样MySQL才有可能使用索引来优化。要理解优化关联查询的第一个技巧，就需要理解MySQL是如何执行关联查询的。当前MySQL关联执行的策略非常简单，它对任何的关联都执行嵌套循环关联操作，即先在一个表中循环取出单条数据，然后在嵌套循环到下一个表中寻找匹配的行，依次下去，直到找到所有表中匹配的行为为止。然后根据各个表匹配的行，返回查询中需要的各个列。太抽象了？以上面的示例来说明，比如有这样的一个查询：SELECT A.xx,B.yy WHERE A.xx IN (5,6)假设MySQL按照查询中的关联顺序A、B来进行关联操作，那么可以用下面的伪代码表示MySQL如何完成这个查询：outer_iterator = SELECT A.xx,A.c FROM A WHERE A.xx IN (5,6);while(outer_row) {    inner_row = inner_iterator.next;        output[inner_row.yy,outer_row.xx];    }}可以看到，最外层的查询是根据A.xx列来查询的，A.c上如果有索引的话，整个关联查询也不会使用。再看内层的查询，很明显B.c上如果有索引的话，能够加速查询，因此只需要在关联顺序中的第二张表的相应列上创建索引即可。当需要分页操作时，通常会使用LIMIT加上偏移量的办法实现，同时加上合适的ORDER BY字句。如果有对应的索引，通常效率会不错，否则，MySQL需要做大量的文件排序操作。一个常见的问题是当偏移量非常大的时候，比如：LIMIT 10000 20这样的查询，MySQL需要查询10020条记录然后只返回20条记录，前面的10000条都将被抛弃，这样的代价非常高。优化这种查询一个最简单的办法就是尽可能的使用覆盖索引扫描，而不是查询所有的列。然后根据需要做一次关联查询再返回所有的列。对于偏移量很大时，这样做的效率会提升非常大。考虑下面的查询：SELECT film_id,description FROM film ORDER BY title LIMIT 50,5;如果这张表非常大，那么这个查询最好改成下面的样子：SELECT film.film_id,film.description    SELECT film_id FROM film ORDER BY title LIMIT 50,5这里的延迟关联将大大提升查询效率，让MySQL扫描尽可能少的页面，获取需要访问的记录后在根据关联列回原表查询所需要的列。有时候如果可以使用书签记录上次取数据的位置，那么下次就可以直接从该书签记录的位置开始扫描，这样就可以避免使用OFFSET，比如下面的查询：SELECT id FROM t LIMIT 10000, 10;SELECT id FROM t WHERE id > 10000 LIMIT 10;其他优化的办法还包括使用预先计算的汇总表，或者关联到一个冗余表，冗余表中只包含主键列和需要做排序的列。MySQL处理UNION的策略是先创建临时表，然后再把各个查询结果插入到临时表中，最后再来做查询。因此很多优化策略在UNION查询中都没有办法很好的时候。经常需要手动将WHERE、LIMIT、ORDER BY等字句“下推”到各个子查询中，以便优化器可以充分利用这些条件先优化。除非确实需要服务器去重，否则就一定要使用UNION ALL，如果没有ALL关键字，MySQL会给临时表加上DISTINCT选项，这会导致整个临时表的数据做唯一性检查，这样做的代价非常高。当然即使使用ALL关键字，MySQL总是将结果放入临时表，然后再读出，再返回给客户端。虽然很多时候没有这个必要，比如有时候可以直接把每个子查询的结果返回给客户端。理解查询是如何执行以及时间都消耗在哪些地方，再加上一些优化过程的知识，可以帮助大家更好的理解MySQL，理解常见优化技巧背后的原理。希望本文中的原理、示例能够帮助大家更好的将理论和实践联系起来，更多的将理论知识运用到实践中。其他也没啥说的了，给大家留两个思考题吧，可以在脑袋里想想答案，这也是大家经常挂在嘴边的，但很少有人会思考为什么？[1] 姜承尧 著；MySQL技术内幕-InnoDB存储引擎；机械工业出版社，2013\n[2] Baron Scbwartz 等著；宁海元 周振兴等译；高性能MySQL（第三版）; 电子工业出版社， 2013\n[3] 由 B-/B+树看 MySQL索引结构", "author": "CHEN川", "tags": ["IT技术", "B+Tree", "B-Tree", "优化", "索引"]}
{"url_object_id": "6a9a2f2ed164a2490d1813a3bb30a2b5", "title": "GitHub 工程师：我眼中的理想上司是这样子的", "url": "http://blog.jobbole.com/113956/", "create_date": "2018/05/09", "praise_nums": "1", "favor_nums": 2, "comment_nums": 0, "content": "我是 Github 的一名高级工程师。我不是要找工作，只是一直在思考领导能力的问题，思考在我多年共事过的诸多领导之中，我最欣赏的特质是什么。受到 Chad Fowler 的文章《我想雇什么样的员工》的启发，我也开始留意我想为什么样的领导工作，即——理想的领导是什么样。在分享我的看法之前，先让我简单介绍一下我自己的情况：我是一名经验丰富的工程师，做过很多基础架构的工作，同时在我的专业领域（API 及其生态环境）扮演着技术顾问的角色。我是个不太需要监督指导的人，我老板只要指出问题的大方向，就可以放手让我去完成了。我很乐意解决困难的工程问题，带领团队朝一个方向努力，或是帮助公司与公众就一个项目进行沟通。正如一个同事所言：我就是“擅长搞定麻烦事”。以下是我认为作为一个理想的领导者应有的特质：在工作中明显地表现出冷静和自在，了解你的态度和行为会对周围的人产生哪些影响，非常关注如何营造出一种相互支持的工作环境。工作是生活的一部分，拥有健康的工作时间，会休假。即使你自己选择在常规工作时间以外工作，也不期待别人和你一样，不干扰其他人的工作习惯。无论与谁谈话都在场。善于倾听。基于自己和团队的价值，会经过深思熟虑精心设计工作流程，因为你重视他人的参与和时间，因此不会为了走流程而增加流程。当你要提出批评性的建议的时候，会及时并且私下沟通。会提供具体的细节，并给出改进的建议和所需的支持。当你有积极的反馈意见时，也会提供具体的细节，并以别人喜欢被认可的方式分享出来。足够自信，乐于接受其他人对你工作和方法的反馈。足够谦虚，在你有不懂的时候、犯错的时候或是学到新东西的时候随时承认。享受从周围人身上学习新知识的过程。对公司的情况有深刻透彻的理解，利用已知的信息指导员工如何工作可以创造出最大的价值，为用户和公司带来最好的影响。不害怕质疑和否认自己的领导，或是挑战公司的现状。允许并支持你的员工在划定的范围内自己做决定，即使你可能不会做同样的决定。无论是大型还是小型的任务、新特性还是常规维护，重视他人的工作。经常强调这一点，让每个人都知道你重视他和他的工作成果。无论在何时何地都能培养一种信任的文化。能够意识到使没受重视的人群被边缘化的行为，即使这些行为是微不足道的，也可能是出于潜意识的。意识到这些事所造成的情感上的伤害。当这种情况出现时，能迅速采取行动解决问题。知道科技领域实际上并不是精英政治（根据个人才能和功绩分配权力），多留意在工作中谁创造了最多的价值，谁最积极主动，确保他们受到关注，拥有更多特权。同样，留意谁在工作中遇到了困难，努力纠正其中的不平等。观察大家在日常沟通和正式场合谈论事情有何不同。知道存在对女性和有色人种的系统性偏见，努力保证你的员工在相处和评估时不受到此类歧视。利用自己的特权帮助员工成长，积极赞助员工：使他们融入团队，看到他们的努力，让他们得到提拔。不只是期待员工做出最完美的工作，还要信任并赋予他们做好工作的权力。给予员工接受并完成新挑战所需的支持。不接受平庸。如果你尽了最大的努力，但是员工还是滥竽充数，那他们必须离开。尽管承担着很多责任和利益相关，仍然着眼于如何让员工更轻松地产出最佳工作成果。能注意到有人在工作中感到沮丧、无聊或是很吃力。无论他们主动告诉你的还是你感觉到的，抽出点时间关心他们，倾听其中的原因。从我的观点来看，以上这些都很重要。如果我描述的这些你都符合，我想你会成为一个好榜样，培养出一个互相支持，态度积极而又忠诚的团队。你能让员工的生活变得更好，理由很简单，你真正地关心着你的员工。", "author": "伯乐在线", "tags": ["职场", "职场"]}
{"url_object_id": "00300456460ea6d4134a02e6861bd127", "title": "MySQL 在并发场景下的问题及解决思路", "url": "http://blog.jobbole.com/113968/", "create_date": "2018/05/11", "praise_nums": "1", "favor_nums": 2, "comment_nums": 1, "content": "1、背景对于数据库系统来说在多用户并发条件下提高并发性的同时又要保证数据的一致性一直是数据库系统追求的目标，既要满足大量并发访问的需求又必须保证在此条件下数据的安全，为了满足这一目标大多数数据库通过锁和事务机制来实现，MySQL数据库也不例外。尽管如此我们仍然会在业务开发过程中遇到各种各样的疑难问题，本文将以案例的方式演示常见的并发问题并分析解决思路。2、表锁导致的慢查询的问题首先我们看一个简单案例，根据ID查询一条用户信息：mysql> select * from user where id=6;这个表的记录总数为3条，但却执行了13秒。出现这种问题我们首先想到的是看看当前MySQL进程状态：从进程上可以看出select语句是在等待一个表锁，那么这个表锁又是什么查询产生的呢？这个结果中并没有显示直接的关联关系，但我们可以推测多半是那条update语句产生的（因为进程中没有其他可疑的SQL），为了印证我们的猜测，先检查一下user表结构：果然user表使用了MyISAM存储引擎，MyISAM在执行操作前会产生表锁，操作完成再自动解锁。如果操作是写操作，则表锁类型为写锁，如果操作是读操作则表锁类型为读锁。正如和你理解的一样写锁将阻塞其他操作(包括读和写)，这使得所有操作变为串行；而读锁情况下读-读操作可以并行，但读-写操作仍然是串行。以下示例演示了显式指定了表锁（读锁），读-读并行，读-写串行的情况。显式开启/关闭表锁，使用lock table user read/write; unlock tables;session1:session2：可以看到会话1启用表锁（读锁）执行读操作，这时会话2可以并行执行读操作，但写操作被阻塞。接着看：session1:session2:当session1执行解锁后，seesion2则立刻开始执行写操作，即读-写串行。总结：到此我们把问题的原因基本分析清楚，总结一下——MyISAM存储引擎执行操作时会产生表锁，将影响其他用户对该表的操作，如果表锁是写锁，则会导致其他用户操作串行，如果是读锁则其他用户的读操作可以并行。所以有时我们遇到某个简单的查询花了很长时间，看看是不是这种情况。解决办法：1）、尽量不用MyISAM存储引擎，在MySQL8.0版本中已经去掉了所有的MyISAM存储引擎的表，推荐使用InnoDB存储引擎。2）、如果一定要用MyISAM存储引擎，减少写操作的时间；3、线上修改表结构有哪些风险？如果有一天业务系统需要增大一个字段长度，能否在线上直接修改呢？在回答这个问题前，我们先来看一个案例：以上语句尝试修改user表的name字段长度，语句被阻塞。按照惯例，我们检查一下当前进程：从进程可以看出alter语句在等待一个元数据锁，而这个元数据锁很可能是上面这条select语句引起的，事实正是如此。在执行DML（select、update、delete、insert）操作时，会对表增加一个元数据锁，这个元数据锁是为了保证在查询期间表结构不会被修改，因此上面的alter语句会被阻塞。那么如果执行顺序相反，先执行alter语句，再执行DML语句呢？DML语句会被阻塞吗？例如我正在线上环境修改表结构，线上的DML语句会被阻塞吗？答案是：不确定。在MySQL5.6开始提供了online ddl功能，允许一些DDL语句和DML语句并发，在当前5.7版本对online ddl又有了增强，这使得大部分DDL操作可以在线进行。详见：https://dev.mysql.com/doc/refman/5.7/en/innodb-create-index-overview.html所以对于特定场景执行DDL过程中，DML是否会被阻塞需要视场景而定。总结：通过这个例子我们对元数据锁和online ddl有了一个基本的认识，如果我们在业务开发过程中有在线修改表结构的需求，可以参考以下方案：1、尽量在业务量小的时间段进行；2、查看官方文档，确认要做的表修改可以和DML并发，不会阻塞线上业务；3、推荐使用percona公司的pt-online-schema-change工具，该工具被官方的online ddl更为强大，它的基本原理是：通过insert… select…语句进行一次全量拷贝，通过触发器记录表结构变更过程中产生的增量，从而达到表结构变更的目的。例如要对A表进行变更，主要步骤为：\n\n\n4、一个死锁问题的分析在线上环境下死锁的问题偶有发生，死锁是因为两个或多个事务相互等待对方释放锁，导致事务永远无法终止的情况。为了分析问题，我们下面将模拟一个简单死锁的情况，然后从中总结出一些分析思路。演示环境：MySQL5.7.20 事务隔离级别：RR表user：CREATE TABLE `USER` (`NAME` VARCHAR(300) DEFAULT NULL,PRIMARY KEY (`ID`)下面演示事务1、事务2工作的情况：这是一个简单的死锁场景，事务1、事务2彼此等待对方释放锁，InnoDB存储引擎检测到死锁发生，让事务2回滚，这使得事务1不再等待事务B的锁，从而能够继续执行。那么InnoDB存储引擎是如何检测到死锁的呢？为了弄明白这个问题，我们先检查此时InnoDB的状态：show engine innodb statusG————————\nLATEST DETECTED DEADLOCK\n————————\n2018-01-14 12:17:13 0x70000f1cc000\n*** (1) TRANSACTION:\nTRANSACTION 5120, ACTIVE 17 sec starting index read\nmysql tables in use 1, locked 1\nLOCK WAIT 3 lock struct(s), heap size 1136, 2 row lock(s)\nMySQL thread id 10, OS thread handle 123145556967424, query id 2764 localhost root updating\nupdate user set name=’haha’ where id=4\n*** (1) WAITING FOR THIS LOCK TO BE GRANTED:\nRECORD LOCKS space id 94 page no 3 n bits 80 index PRIMARY of table test.user trx id 5120 lock_mode X locks rec but not gap waiting\nRecord lock, heap no 5 PHYSICAL RECORD: n_fields 5; compact format; info bits 0\n0: len 4; hex 80000004; asc ;;\n1: len 6; hex 0000000013fa; asc ;;\n2: len 7; hex 520000060129a6; asc R ) ;;\n3: len 4; hex 68616861; asc haha;;\n4: len 4; hex 80000015; asc ;;*** (2) TRANSACTION:\nTRANSACTION 5121, ACTIVE 12 sec starting index read\nmysql tables in use 1, locked 1\n3 lock struct(s), heap size 1136, 2 row lock(s)\nMySQL thread id 11, OS thread handle 123145555853312, query id 2765 localhost root updating\nupdate user set name=’hehe’ where id=3\n*** (2) HOLDS THE LOCK(S):\nRECORD LOCKS space id 94 page no 3 n bits 80 index PRIMARY of table test.user trx id 5121 lock_mode X locks rec but not gap\nRecord lock, heap no 5 PHYSICAL RECORD: n_fields 5; compact format; info bits 0\n0: len 4; hex 80000004; asc ;;\n1: len 6; hex 0000000013fa; asc ;;\n2: len 7; hex 520000060129a6; asc R ) ;;\n3: len 4; hex 68616861; asc haha;;\n4: len 4; hex 80000015; asc ;;*** (2) WAITING FOR THIS LOCK TO BE GRANTED:\nRECORD LOCKS space id 94 page no 3 n bits 80 index PRIMARY of table test.user trx id 5121 lock_mode X locks rec but not gap waiting\nRecord lock, heap no 7 PHYSICAL RECORD: n_fields 5; compact format; info bits 0\n0: len 4; hex 80000003; asc ;;\n1: len 6; hex 0000000013fe; asc ;;\n2: len 7; hex 5500000156012f; asc U V /;;\n3: len 4; hex 68656865; asc hehe;;\n4: len 4; hex 80000014; asc ;;*** WE ROLL BACK TRANSACTION (2)InnoDB状态有很多指标，这里我们截取死锁相关的信息，可以看出InnoDB可以输出最近出现的死锁信息，其实很多死锁监控工具也是基于此功能开发的。在死锁信息中，显示了两个事务等待锁的相关信息（蓝色代表事务1、绿色代表事务2），重点关注：WAITING FOR THIS LOCK TO BE GRANTED和HOLDS THE LOCK(S)。WAITING FOR THIS LOCK TO BE GRANTED表示当前事务正在等待的锁信息，从输出结果看出事务1正在等待heap no为5的行锁，事务2正在等待 heap no为7的行锁；HOLDS THE LOCK(S)：表示当前事务持有的锁信息，从输出结果看出事务2持有heap no为5行锁。从输出结果看出，最后InnoDB回滚了事务2。那么InnoDB是如何检查出死锁的呢？我们想到最简单方法是假如一个事务正在等待一个锁，如果等待时间超过了设定的阈值，那么该事务操作失败，这就避免了多个事务彼此长等待的情况。参数innodb_lock_wait_timeout正是用来设置这个锁等待时间的。如果按照这个方法，解决死锁是需要时间的（即等待超过innodb_lock_wait_timeout设定的阈值），这种方法稍显被动而且影响系统性能，InnoDB存储引擎提供一个更好的算法来解决死锁问题，wait-for graph算法。简单的说，当出现多个事务开始彼此等待时，启用wait-for graph算法，该算法判定为死锁后立即回滚其中一个事务，死锁被解除。该方法的好处是：检查更为主动，等待时间短。下面是wait-for graph算法的基本原理：为了便于理解，我们把死锁看做4辆车彼此阻塞的场景：4辆车看做4个事务，彼此等待对方的锁，造成死锁。wait-for graph算法原理是把事务作为节点，事务之间的锁等待关系，用有向边表示，例如事务A等待事务B的锁，就从节点A画一条有向边到节点B，这样如果A、B、C、D构成的有向图，形成了环，则判断为死锁。这就是wait-for graph算法的基本原理。总结：1、如果我们业务开发中出现死锁如何检查出？刚才已经介绍了通过监控InnoDB状态可以得出，你可以做一个小工具把死锁的记录收集起来，便于事后查看。2、如果出现死锁，业务系统应该如何应对？从上文我们可以看到当InnoDB检查出死锁后，对客户端报出一个Deadlock found when trying to get lock; try restarting transaction信息，并且回滚该事务，应用端需要针对该信息，做事务重启的工作，并保存现场日志事后做进一步分析，避免下次死锁的产生。5、锁等待问题的分析在业务开发中死锁的出现概率较小，但锁等待出现的概率较大，锁等待是因为一个事务长时间占用锁资源，而其他事务一直等待前个事务释放锁。从上述可知事务1长时间持有id=3的行锁，事务2产生锁等待，等待时间超过innodb_lock_wait_timeout后操作中断，但事务并没有回滚。如果我们业务开发中遇到锁等待，不仅会影响性能，还会给你的业务流程提出挑战，因为你的业务端需要对锁等待的情况做适应的逻辑处理，是重试操作还是回滚事务。在MySQL元数据表中有对事务、锁等待的信息进行收集，例如information_schema数据库下的INNODB_LOCKS、INNODB_TRX、INNODB_LOCK_WAITS，你可以通过这些表观察你的业务系统锁等待的情况。你也可以用一下语句方便的查询事务和锁等待的关联关系：SELECT   R.TRX_ID WAITING_TRX_ID,           R.TRX_QUERY WATING_QUERY,           B.TRX_MYSQL_THREAD_ID BLOCKING_THREAD,FROM   INFORMATION_SCHEMA.INNODB_LOCK_ WINNER JOIN   INFORMATION_SCHEMA.INNODB_ R ON R.TRX_ID = W.REQUESTING_TRX_ID;结果：waiting_trx_id: 5132\nwaiting_thread: 11\nwating_query: update user set name=’hehe’ where id=3\nblocking_trx_id: 5133\nblocking_thread: 10\nblocking_query: NULL总结：1、请对你的业务系统做锁等待的监控，这有助于你了解当前数据库锁情况，以及为你优化业务程序提供帮助；2、业务系统中应该对锁等待超时的情况做合适的逻辑判断。6、小结本文通过几个简单的示例介绍了我们常用的几种MySQL并发问题，并尝试得出针对这些问题我们排查的思路。文中涉及事务、表锁、元数据锁、行锁，但引起并发问题的远远不止这些，例如还有事务隔离级别、GAP锁等。真实的并发问题可能多而复杂，但排查思路和方法却是可以复用，在本文中我们使用了show processlist;show engine innodb status;以及查询元数据表的方法来排查发现问题，如果问题涉及到了复制，还需要借助master/slave监控来协助。参考资料：姜承尧《InnoDB存储引擎》李宏哲 杨挺 《MySQL排查指南》何登成 http://hedengcheng.com ", "author": "李平", "tags": ["IT技术", " 1 评论 ", "数据库"]}
{"url_object_id": "97dd29d801309a1f0316b5b23e4fbb18", "title": "分布式之数据库和缓存双写一致性方案解析", "url": "http://blog.jobbole.com/113992/", "create_date": "2018/05/17", "praise_nums": "1", "favor_nums": 0, "comment_nums": 1, "content": "为什么写这篇文章？首先，缓存由于其高并发和高性能的特性，已经在项目中被广泛使用。在读取缓存方面，大家没啥疑问，都是按照下图的流程来进行业务操作。\n\n但是在更新缓存方面，对于更新完数据库，是更新缓存呢，还是删除缓存。又或者是先删除缓存，再更新数据库，其实大家存在很大的争议。目前没有一篇全面的博客，对这几种方案进行解析。于是博主战战兢兢，顶着被大家喷的风险，写了这篇文章。文章结构本文由以下三个部分组成\n1、讲解缓存更新策略\n2、对每种策略进行缺点分析\n3、针对缺点给出改进方案先做一个说明，从理论上来说，给缓存设置过期时间，是保证最终一致性的解决方案。这种方案下，我们可以对存入缓存的数据设置过期时间，所有的写操作以数据库为准，对缓存操作只是尽最大努力即可。也就是说如果数据库写成功，缓存更新失败，那么只要到达过期时间，则后面的读请求自然会从数据库中读取新值然后回填缓存。因此，接下来讨论的思路不依赖于给缓存设置过期时间这个方案。\n在这里，我们讨论三种更新策略：应该没人问我，为什么没有先更新缓存，再更新数据库这种策略。(1)先更新数据库，再更新缓存这套方案，大家是普遍反对的。为什么呢？有如下两点原因。\n原因一（线程安全角度）\n同时有请求A和请求B进行更新操作，那么会出现\n（1）线程A更新了数据库\n（2）线程B更新了数据库\n（3）线程B更新了缓存\n（4）线程A更新了缓存\n这就出现请求A更新缓存应该比请求B更新缓存早才对，但是因为网络等原因，B却比A更早更新了缓存。这就导致了脏数据，因此不考虑。\n原因二（业务场景角度）\n有如下两点：\n（1）如果你是一个写数据库场景比较多，而读数据场景比较少的业务需求，采用这种方案就会导致，数据压根还没读到，缓存就被频繁的更新，浪费性能。\n（2）如果你写入数据库的值，并不是直接写入缓存的，而是要经过一系列复杂的计算再写入缓存。那么，每次写入数据库后，都再次计算写入缓存的值，无疑是浪费性能的。显然，删除缓存更为适合。接下来讨论的就是争议最大的，先删缓存，再更新数据库。还是先更新数据库，再删缓存的问题。(2)先删缓存，再更新数据库该方案会导致不一致的原因是。同时有一个请求A进行更新操作，另一个请求B进行查询操作。那么会出现如下情形:\n（1）请求A进行写操作，删除缓存\n（2）请求B查询发现缓存不存在\n（3）请求B去数据库查询得到旧值\n（4）请求B将旧值写入缓存\n（5）请求A将新值写入数据库\n上述情况就会导致不一致的情形出现。而且，如果不采用给缓存设置过期时间策略，该数据永远都是脏数据。\n那么，如何解决呢？采用延时双删策略\n伪代码如下public void write(String key,Object data){        db.updateData(data);        redis.delKey(key);转化为中文描述就是\n（1）先淘汰缓存\n（2）再写数据库（这两步和原来一样）\n（3）休眠1秒，再次淘汰缓存\n这么做，可以将1秒内所造成的缓存脏数据，再次删除。\n那么，这个1秒怎么确定的，具体该休眠多久呢？\n针对上面的情形，读者应该自行评估自己的项目的读数据业务逻辑的耗时。然后写数据的休眠时间则在读数据业务逻辑的耗时基础上，加几百ms即可。这么做的目的，就是确保读请求结束，写请求可以删除读请求造成的缓存脏数据。\n如果你用了mysql的读写分离架构怎么办？\nok，在这种情况下，造成数据不一致的原因如下，还是两个请求，一个请求A进行更新操作，另一个请求B进行查询操作。\n（1）请求A进行写操作，删除缓存\n（2）请求A将数据写入数据库了，\n（3）请求B查询缓存发现，缓存没有值\n（4）请求B去从库查询，这时，还没有完成主从同步，因此查询到的是旧值\n（5）请求B将旧值写入缓存\n（6）数据库完成主从同步，从库变为新值\n上述情形，就是数据不一致的原因。还是使用双删延时策略。只是，睡眠时间修改为在主从同步的延时时间基础上，加几百ms。\n采用这种同步淘汰策略，吞吐量降低怎么办？\nok，那就将第二次删除作为异步的。自己起一个线程，异步删除。这样，写的请求就不用沉睡一段时间后了，再返回。这么做，加大吞吐量。\n第二次删除,如果删除失败怎么办？\n这是个非常好的问题，因为第二次删除失败，就会出现如下情形。还是有两个请求，一个请求A进行更新操作，另一个请求B进行查询操作，为了方便，假设是单库：\n（1）请求A进行写操作，删除缓存\n（2）请求B查询发现缓存不存在\n（3）请求B去数据库查询得到旧值\n（4）请求B将旧值写入缓存\n（5）请求A将新值写入数据库\n（6）请求A试图去删除请求B写入对缓存值，结果失败了。\nok,这也就是说。如果第二次删除缓存失败，会再次出现缓存和数据库不一致的问题。\n如何解决呢？\n具体解决方案，且看博主对第(3)种更新策略的解析。(3)先更新数据库，再删缓存首先，先说一下。老外提出了一个缓存更新套路，名为《Cache-Aside pattern》。其中就指出：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。：应用程序从cache中取数据，取到后返回。：先把数据存到数据库中，成功后，再让缓存失效。另外，知名社交网站facebook也在论文《Scaling Memcache at Facebook》中提出，他们用的也是先更新数据库，再删缓存的策略。\n这种情况不存在并发问题么？\n不是的。假设这会有两个请求，一个请求A做查询操作，一个请求B做更新操作，那么会有如下情形产生\n（1）缓存刚好失效\n（2）请求A查询数据库，得一个旧值\n（3）请求B将新值写入数据库\n（4）请求B删除缓存\n（5）请求A将查到的旧值写入缓存\nok，如果发生上述情况，确实是会发生脏数据。\n然而，发生这种情况的概率又有多少呢？\n发生上述情况有一个先天性条件，就是步骤（3）的写数据库操作比步骤（2）的读数据库操作耗时更短，才有可能使得步骤（4）先于步骤（5）。可是，大家想想，数据库的读操作的速度远快于写操作的（不然做读写分离干嘛，做读写分离的意义就是因为读操作比较快，耗资源少），因此步骤（3）耗时比步骤（2）更短，这一情形很难出现。\n假设，有人非要抬杠，有强迫症，一定要解决怎么办？\n如何解决上述并发问题？\n首先，给缓存设有效时间是一种方案。其次，采用策略（2）里给出的异步延时删除策略，保证读请求完成以后，再进行删除操作。\n还有其他造成不一致的原因么？\n有的，这也是缓存更新策略（2）和缓存更新策略（3）都存在的一个问题，如果删缓存失败了怎么办，那不是会有不一致的情况出现么。比如一个写数据请求，然后写入数据库了，删缓存失败了，这会就出现不一致的情况了。这也是缓存更新策略（2）里留下的最后一个疑问。\n如何解决？\n提供一个保障的重试机制即可，这里给出两套方案。\n方案一：\n如下图所示\n\n流程如下所示\n（1）更新数据库数据；\n（2）缓存因为种种问题删除失败\n（3）将需要删除的key发送至消息队列\n（4）自己消费消息，获得需要删除的key\n（5）继续重试删除操作，直到成功\n然而，该方案有一个缺点，对业务线代码造成大量的侵入。于是有了方案二，在方案二中，启动一个订阅程序去订阅数据库的binlog，获得需要操作的数据。在应用程序中，另起一段程序，获得这个订阅程序传来的信息，进行删除缓存操作。\n方案二：\n\n流程如下图所示：\n（1）更新数据库数据\n（2）数据库会将操作信息写入binlog日志当中\n（3）订阅程序提取出所需要的数据以及key\n（4）另起一段非业务代码，获得该信息\n（5）尝试删除缓存操作，发现删除失败\n（6）将这些信息发送至消息队列\n（7）重新从消息队列中获得该数据，重试操作。备注说明：上述的订阅binlog程序在mysql中有现成的中间件叫canal，可以完成订阅binlog日志的功能。至于oracle中，博主目前不知道有没有现成中间件可以使用。另外，重试机制，博主是采用的是消息队列的方式。如果对一致性要求不是很高，直接在程序中另起一个线程，每隔一段时间去重试即可，这些大家可以灵活自由发挥，只是提供一个思路。本文其实是对目前互联网中已有的一致性方案，进行了一个总结。对于先删缓存，再更新数据库的更新策略，还有方案提出维护一个内存队列的方式，博主看了一下，觉得实现异常复杂，没有必要，因此没有必要在文中给出。最后，希望大家有所收获。1、主从DB与cache一致性\n2、缓存更新的套路", "author": "孤独烟", "tags": ["IT技术", " 1 评论 ", "分布式", "数据库"]}
{"url_object_id": "4e10d4da22dcb529fe4ac299ec0ce8f8", "title": "终于有人把云计算、大数据和人工智能讲明白了", "url": "http://blog.jobbole.com/114005/", "create_date": "2018/05/19", "praise_nums": "3", "favor_nums": 5, "comment_nums": 0, "content": "我今天要讲这三个话题，一个是云计算，一个大数据，一个人工智能，我为什么要讲这三个东西呢？因为这三个东西现在非常非常的火，它们之间好像互相有关系，一般谈云计算的时候也会提到大数据，谈人工智能的时候也会提大数据，谈人工智能的时候也会提云计算。所以说感觉他们又相辅相成不可分割，如果是非技术的人员来讲可能比较难理解说这三个之间的相互关系，所以有必要解释一下。一、云计算最初是实现资源管理的灵活性我们首先来说云计算，云计算最初的目标是对资源的管理，管理的主要是计算资源，网络资源，存储资源三个方面。1.1 管数据中心就像配电脑什么叫计算，网络，存储资源呢？就说你要买台笔记本电脑吧，你是不是要关心这台电脑什么样的CPU啊？多大的内存啊？这两个我们称为计算资源。这台电脑要能上网吧，需要有个网口可以插网线，或者有无线网卡可以连接我们家的路由器，您家也需要到运营商比如联通，移动，电信开通一个网络，比如100M的带宽，然后会有师傅弄一根网线到您家来，师傅可能会帮您将您的路由器和他们公司的网络连接配置好，这样您家的所有的电脑，手机，平板就都可以通过您的路由器上网了。这就是网络。您可能还会问硬盘多大啊？原来硬盘都很小，10G之类的，后来500G，1T，2T的硬盘也不新鲜了。(1T是1000G)，这就是存储。对于一台电脑是这个样子的，对于一个数据中心也是同样的。想象你有一个非常非常大的机房，里面堆了很多的服务器，这些服务器也是有CPU，内存，硬盘的，也是通过类似路由器的设备上网的。这个时候的一个问题就是，运营数据中心的人是怎么把这些设备统一的管理起来的呢？1.2 灵活就是想啥时要都有，想要多少都行管理的目标就是要达到两个方面的灵活性。哪两个方面呢？比如有个人需要一台很小很小的电脑，只有一个CPU，1G内存，10G的硬盘，一兆的带宽，你能给他吗？像这种这么小规格的电脑，现在随便一个笔记本电脑都比这个配置强了，家里随便拉一个宽带都要100M。然而如果去一个云计算的平台上，他要想要这个资源的时候，只要一点就有了。所以说它就能达到两个方面灵活性。第一个方面就是想什么时候要就什么时候要，比如需要的时候一点就出来了，这个叫做时间灵活性。第二个方面就是想要多少呢就有多少，比如需要一个很小很小的电脑，可以满足，比如需要一个特别大的空间，以云盘为例，似乎云盘给每个人分配的空间动不动就就很大很大，随时上传随时有空间，永远用不完，这个叫做空间灵活性。空间灵活性和时间灵活性，也即我们常说的云计算的弹性。为了解决这个弹性的问题，经历了漫长时间的发展。1.3 物理设备不灵活首先第一个阶段就是物理机，或者说物理设备时期。这个时期相当于客户需要一台电脑，我们就买一台放在数据中心里。物理设备当然是越来越牛，例如服务器，内存动不动就是百G内存，例如网络设备，一个端口的带宽就能有几十G甚至上百G，例如存储，在数据中心至少是PB级别的(一个P是1000个T，一个T是1000个G)。然而物理设备不能做到很好的灵活性。首先它不能够达到想什么时候要就什么时候要、比如买台服务器，哪怕买个电脑，都有采购的时间。突然用户告诉某个云厂商，说想要开台电脑，如果使用物理服务器，当时去采购啊就很难，如果说供应商啊关系一般，可能采购一个月，供应商关系好的话也需要一个星期。用户等了一个星期后，这时候电脑才到位，用户还要登录上去开始慢慢部署自己的应用，时间灵活性非常差。第二是空间灵活性也不行，例如上述的用户，要一个很小很小的电脑，现在哪还有这么小型号的电脑啊。不能为了满足用户只要一个G的内存是80G硬盘的，就去买一个这么小的机器。但是如果买一个大的呢，因为电脑大，就向用户多收钱，用户说他只用这么小的一点，如果让用户多付钱就很冤。1.4 虚拟化灵活多了有人就想办法了。第一个办法就是虚拟化。用户不是只要一个很小的电脑么？数据中心的物理设备都很强大，我可以从物理的CPU，内存，硬盘中虚拟出一小块来给客户，同时也可以虚拟出一小块来给其他客户，每个客户都只能看到自己虚的那一小块，其实每个客户用的是整个大的设备上其中的一小块。虚拟化的技术能使得不同的客户的电脑看起来是隔离的，我看着好像这块盘就是我的，你看这呢这块盘就是你的，实际情况可能我这个10G和您这个10G是落在同样一个很大很大的这个存储上的。而且如果事先物理设备都准备好，虚拟化软件虚拟出一个电脑是非常快的，基本上几分钟就能解决。所以在任何一个云上要创建一台电脑，一点几分钟就出来了，就是这个道理。这个空间灵活性和时间灵活性就基本解决了。1.5 虚拟世界的赚钱与情怀在虚拟化阶段，最牛的公司是Vmware，是实现虚拟化技术比较早的一家公司，可以实现计算，网络，存储的虚拟化，这家公司很牛，性能也做得非常好，然后虚拟化软件卖的也非常好，赚了好多的钱，后来让EMC(世界五百强，存储厂商第一品牌)给收购了。但是这个世界上还是有很多有情怀的人的，尤其是程序员里面，有情怀的人喜欢做一件什么事情呢？开源。这个世界上很多软件都是有闭源就有开源，源就是源代码。就是说某个软件做的好，所有人都爱用，这个软件的代码呢，我封闭起来只有我公司知道，其他人不知道，如果其他人想用这个软件，就要付我钱，这就叫闭源。但是世界上总有一些大牛看不惯钱都让一家赚了去。大牛们觉得，这个技术你会我也会，你能开发出来，我也能，我开发出来就是不收钱，把代码拿出来分享给大家，全世界谁用都可以，所有的人都可以享受到好处，这个叫做开源。比如最近蒂姆·伯纳斯·李就是个非常有情怀的人，2017年，他因“发明万维网、第一个浏览器和使万维网得以扩展的基本协议和算法”而获得2016年度的图灵奖。图灵奖就是计算机界的诺贝尔奖。然而他最令人敬佩的是，他将万维网，也就是我们常见的www的技术无偿贡献给全世界免费使用。我们现在在网上的所有行为都应该感谢他的功劳，如果他将这个技术拿来收钱，应该和比尔盖茨差不多有钱。例如在闭源的世界里有windows，大家用windows都得给微软付钱，开源的世界里面就出现了Linux。比尔盖茨靠windows，Office这些闭源的软件赚了很多钱，称为世界首富，就有大牛开发了另外一种操作系统Linux。很多人可能没有听说过Linux，很多后台的服务器上跑的程序都是Linux上的，比如大家享受双十一，支撑双十一抢购的系统，无论是淘宝，京东，考拉，都是跑在Linux上的。再如有apple就有安卓。apple市值很高，但是苹果系统的代码我们是看不到的。于是就有大牛写了安卓手机操作系统。所以大家可以看到几乎所有的其他手机厂商，里面都装安卓系统，因为苹果系统不开源，而安卓系统大家都可以用。在虚拟化软件也一样，有了Vmware，这个软件非常非常的贵。那就有大牛写了两个开源的虚拟化软件，一个叫做Xen，一个叫做KVM，如果不做技术的，可以不用管这两个名字，但是后面还是会提到。1.6 虚拟化的半自动和云计算的全自动虚拟化软件似乎解决了灵活性问题，其实不全对。因为虚拟化软件一般创建一台虚拟的电脑，是需要人工指定这台虚拟电脑放在哪台物理机上的，可能还需要比较复杂的人工配置，所以使用Vmware的虚拟化软件，需要考一个很牛的证书，能拿到这个证书的人，薪资是相当的高，也可见复杂程度。所以仅仅凭虚拟化软件所能管理的物理机的集群规模都不是特别的大，一般在十几台，几十台，最多百台这么一个规模。这一方面会影响时间灵活性，虽然虚拟出一台电脑的时间很短，但是随着集群规模的扩大，人工配置的过程越来越复杂，越来越耗时。另一方面也影响空间灵活性，当用户数量多的时候，这点集群规模，还远达不到想要多少要多少的程度，很可能这点资源很快就用完了，还得去采购。所以随着集群的规模越来越大，基本都是千台起步，动辄上万台，甚至几十上百万台，如果去查一下BAT，包括网易，包括谷歌，亚马逊，服务器数目都大的吓人。这么多机器要靠人去选一个位置放这台虚拟化的电脑并做相应的配置，几乎是不可能的事情，还是需要机器去做这个事情。人们发明了各种各样的算法来做这个事情，算法的名字叫做调度(Scheduler)。通俗一点的说，就是有一个调度中心，几千台机器都在一个池子里面，无论用户需要多少CPU，内存，硬盘的虚拟电脑，调度中心会自动在大池子里面找一个能够满足用户需求的地方，把虚拟电脑启动起来做好配置，用户就直接能用了。这个阶段，我们称为池化，或者云化，到了这个阶段，才可以称为云计算，在这之前都只能叫虚拟化。1.7 云计算的私有与公有云计算大致分两种，一个是私有云，一个是公有云，还有人把私有云和公有云连接起来称为混合云，我们暂且不说这个。私有云就是把虚拟化和云化的这套软件部署在别人的数据中心里面，使用私有云的用户往往很有钱，自己买地建机房，自己买服务器，然后让云厂商部署在自己这里，Vmware后来除了虚拟化，也推出了云计算的产品，并且在私有云市场赚的盆满钵满。所谓公有云就是虚拟化和云化软件部署在云厂商自己数据中心里面的，用户不需要很大的投入，只要注册一个账号，就能在一个网页上点一下创建一台虚拟电脑，例如AWS也即亚马逊的公有云，例如国内的阿里云，腾讯云，网易云等。亚马逊呢为什么要做公有云呢？我们知道亚马逊原来是国外比较大的一个电商，它做电商的时候也肯定会遇到类似双11的场景，在某一个时刻大家都冲上来买东西。当大家都冲上买东西的时候，就特别需要云的时间灵活性和空间灵活性。因为它不能时刻准备好所有的资源，那样太浪费了。但也不能什么都不准备，看着双十一这么多用户想买东西登不上去。所以需要双十一的时候，创建一大批虚拟电脑来支撑电商应用，过了双十一再把这些资源都释放掉去干别的。所以亚马逊是需要一个云平台的。然而商用的虚拟化软件实在是太贵了，亚马逊总不能把自己在电商赚的钱全部给了虚拟化厂商吧。于是亚马逊基于开源的虚拟化技术，如上所述的Xen或者KVM，开发了一套自己的云化软件。没想到亚马逊后来电商越做越牛，云平台也越做越牛。而且由于他的云平台需要支撑自己的电商应用，而传统的云计算厂商多为IT厂商出身，几乎没有自己的应用，因而亚马逊的云平台对应用更加的友好，迅速发展成为云计算的第一品牌，赚了很多钱。在亚马逊公布其云计算平台财报之前，人们都猜测，亚马逊电商赚钱，云也赚钱吗？后来一公布财报，发现不是一般的赚钱，仅仅去年，亚马逊AWS年营收达122亿美元，运营利润31亿美元。1.8 云计算的赚钱与情怀公有云的第一名亚马逊过得很爽，第二名Rackspace过的就一般了。没办法，这就是互联网行业的残酷性，多是赢者通吃的模式。所以第二名如果不是云计算行业的，很多人可能都没听过了。第二名就想，我干不过老大怎么办呢？开源吧。如上所述，亚马逊虽然使用了开源的虚拟化技术，但是云化的代码是闭源的，很多想做又做不了云化平台的公司，只能眼巴巴的看着亚马逊挣大钱。Rackspace把源代码一公开，整个行业就可以一起把这个平台越做越好，兄弟们大家一起上，和老大拼了。于是Rackspace和美国航空航天局合作创办了开源软件OpenStack，如图所示OpenStack的架构图，不是云计算行业的不用弄懂这个图，但是能够看到三个关键字，Compute计算，Networking网络，Storage存储。还是一个计算，网络，存储的云化管理平台。当然第二名的技术也是非常棒的，有了OpenStack之后，果真像Rackspace想象的一样，所有想做云的大企业都疯了，你能想象到的所有如雷贯耳的大型IT企业，IBM，惠普，戴尔，华为，联想等等，都疯了。原来云平台大家都想做，看着亚马逊和Vmware赚了这么多钱，眼巴巴看着没办法，想自己做一个好像难度还挺大。现在好了，有了这样一个开源的云平台OpenStack，所有的IT厂商都加入到这个社区中来，对这个云平台进行贡献，包装成自己的产品，连同自己的硬件设备一起卖。有的做了私有云，有的做了公有云，OpenStack已经成为开源云平台的事实标准。1.9 IaaS, 资源层面的灵活性随着OpenStack的技术越来越成熟，可以管理的规模也越来越大，并且可以有多个OpenStack集群部署多套，比如北京部署一套，杭州部署两套，广州部署一套，然后进行统一的管理。这样整个规模就更大了。在这个规模下，对于普通用户的感知来讲，基本能够做到想什么时候要就什么什么药，想要多少就要多少。还是拿云盘举例子，每个用户云盘都分配了5T甚至更大的空间，如果有1亿人，那加起来空间多大啊。其实背后的机制是这样的，分配你的空间，你可能只用了其中很少一点，比如说它分配给你了5个T，这么大的空间仅仅是你看到的，而不是真的就给你了，你其实只用了50个G，则真实给你的就是50个G，随着你文件的不断上传，分给你的空间会越来越多。当大家都上传，云平台发现快满了的时候(例如用了70%)，会采购更多的服务器，扩充背后的资源，这个对用户是透明的，看不到的，从感觉上来讲，就实现了云计算的弹性。其实有点像银行，给储户的感觉是什么时候取钱都有，只要不同时挤兑，银行就不会垮。这里做一个简单的总结，到了这个阶段，云计算基本上实现了时间灵活性和空间灵活性，实现了计算，网络，存储资源的弹性。计算，网络，存储我们常称为基础设施Infranstracture, 因而这个阶段的弹性称为资源层面的弹性，管理资源的云平台，我们称为基础设施服务，就是我们常听到的IaaS，Infranstracture As A Service。二、云计算不光管资源，也要管应用有了IaaS，实现了资源层面的弹性就够了吗？显然不是。还有应用层面的弹性。这里举个例子，比如说实现一个电商的应用，平时十台机器就够了，双十一需要一百台。你可能觉得很好办啊，有了IaaS，新创建九十台机器就可以了啊。但是90台机器创建出来是空的啊，电商应用并没有放上去啊，只能你公司的运维人员一台一台的弄，还是需要很长时间才能安装好的。虽然资源层面实现了弹性，但是没有应用层的弹性，依然灵活性是不够的。有没有方法解决这个问题呢？于是人们在IaaS平台之上又加了一层，用于管理资源以上的应用弹性的问题，这一层通常称为PaaS（Platform As A Service）。这一层往往比较难理解，其实大致分两部分，一部分我称为你自己的应用自动安装，一部分我称为通用的应用不用安装。我们先来说第一部分，自己的应用自动安装。比如电商应用是你自己开发的，除了你自己，其他人是不知道怎么安装的，比如电商应用，安装的时候需要配置支付宝或者微信的账号，才能别人在你的电商上买东西的时候，付的钱是打到你的账户里面的，除了你，谁也不知道，所以安装的过程平台帮不了忙，但是能够帮你做的自动化，你需要做一些工作，将自己的配置信息融入到自动化的安装过程中方可。比如上面的例子，双十一新创建出来的90台机器是空的，如果能够提供一个工具，能够自动在这新的90台机器上将电商应用安装好，就能够实现应用层面的真正弹性。例如Puppet, Chef, Ansible, Cloud Foundary都可以干这件事情，最新的容器技术Docker能更好的干这件事情，不做技术的可以不用管这些词。第二部分，通用的应用不用安装。所谓通用的应用，一般指一些复杂性比较高，但是大家都在用的，例如数据库。几乎所有的应用都会用数据库，但是数据库软件是标准的，虽然安装和维护比较复杂，但是无论谁安装都是一样。这样的应用可以变成标准的PaaS层的应用放在云平台的界面上。当用户需要一个数据库的时候，一点就出来了，用户就可以直接用了。有人问，既然谁安装都一个样，那我自己来好了，不需要花钱在云平台上买。当然不是，数据库是一个非常难的东西，光Oracle这家公司，靠数据库就能赚这么多钱。买Oracle也是要花很多很多钱的。然而大多数云平台会提供Mysql这样的开源数据库，又是开源，钱不需要花这么多了，但是维护这个数据库，却需要专门招一个很大的团队，如果这个数据库能够优化到能够支撑双十一，也不是一年两年能够搞定的。比如您是一个做单车的，当然没必要招一个非常大的数据库团队来干这件事情，成本太高了，应该交给云平台来做这件事情，专业的事情专业的人来自，云平台专门养了几百人维护这套系统，您只要专注于您的单车应用就可以了。要么是自动部署，要么是不用部署，总的来说就是应用层你也要少操心，这就是PaaS层的重要作用。虽说脚本的方式能够解决自己的应用的部署问题，然而不同的环境千差万别，一个脚本往往在一个环境上运行正确，到另一个环境就不正确了。而容器是能更好的解决这个问题的。容器是 Container，Container另一个意思是集装箱，其实容器的思想就是要变成软件交付的集装箱。集装箱的特点，一是封装，二是标准。在没有集装箱的时代，假设将货物从 A运到 B，中间要经过三个码头、换三次船。每次都要将货物卸下船来，摆的七零八落，然后搬上船重新整齐摆好。因此在没有集装箱的时候，每次换船，船员们都要在岸上待几天才能走。有了集装箱以后，所有的货物都打包在一起了，并且集装箱的尺寸全部一致，所以每次换船的时候，一个箱子整体搬过去就行了，小时级别就能完成，船员再也不用上岸长时间耽搁了。这是集装箱“封装”、“标准”两大特点在生活中的应用。那么容器如何对应用打包呢？还是要学习集装箱，首先要有个封闭的环境，将货物封装起来，让货物之间互不干扰，互相隔离，这样装货卸货才方便。好在 Ubuntu中的LXC技术早就能做到这一点。封闭的环境主要使用了两种技术，一种是看起来是隔离的技术，称为 Namespace，也即每个 Namespace中的应用看到的是不同的 IP地址、用户空间、程号等。另一种是用起来是隔离的技术，称为 Cgroups，也即明明整台机器有很多的 CPU、内存，而一个应用只能用其中的一部分。所谓的镜像，就是将你焊好集装箱的那一刻，将集装箱的状态保存下来，就像孙悟空说：“定”，集装箱里面就定在了那一刻，然后将这一刻的状态保存成一系列文件。这些文件的格式是标准的，谁看到这些文件都能还原当时定住的那个时刻。将镜像还原成运行时的过程（就是读取镜像文件，还原那个时刻的过程）就是容器运行的过程。有了容器，使得 PaaS层对于用户自身应用的自动部署变得快速而优雅。三、大数据拥抱云计算在PaaS层中一个复杂的通用应用就是大数据平台。大数据是如何一步一步融入云计算的呢？3.1 数据不大也包含智慧一开始这个大数据并不大，你想象原来才有多少数据？现在大家都去看电子书，上网看新闻了，在我们80后小时候，信息量没有那么大，也就看看书，看看报，一个星期的报纸加起来才有多少字啊，如果你不在一个大城市，一个普通的学校的图书馆加起来也没几个书架，是后来随着信息化的到来，信息才会越来越多。首先我们来看一下大数据里面的数据，就分三种类型，一种叫结构化的数据，一种叫非结构化的数据，还有一种叫半结构化的数据。什么叫结构化的数据呢？叫有固定格式和有限长度的数据。例如填的表格就是结构化的数据，国籍：中华人民共和国，民族：汉，性别：男，这都叫结构化数据。现在越来越多的就是非结构化的数据，就是不定长，无固定格式的数据，例如网页，有时候非常长，有时候几句话就没了，例如语音，视频都是非结构化的数据。半结构化数据是一些xml或者html的格式的，不从事技术的可能不了解，但也没有关系。数据怎么样才能对人有用呢？其实数据本身不是有用的，必须要经过一定的处理。例如你每天跑步带个手环收集的也是数据，网上这么多网页也是数据，我们称为Data，数据本身没有什么用处，但是数据里面包含一个很重要的东西，叫做信息Information，数据十分杂乱，经过梳理和清洗，才能够称为信息。信息会包含很多规律，我们需要从信息中将规律总结出来，称为知识knowledge，知识改变命运。信息是很多的，但是有人看到了信息相当于白看，但是有人就从信息中看到了电商的未来，有人看到了直播的未来，所以人家就牛了，你如果没有从信息中提取出知识，天天看朋友圈，也只能在互联网滚滚大潮中做个看客。有了知识，然后利用这些知识去应用于实战，有的人会做得非常好，这个东西叫做智慧intelligence。有知识并不一定有智慧，例如好多学者很有知识，已经发生的事情可以从各个角度分析的头头是道，但一到实干就歇菜，并不能转化成为智慧。而很多的创业家之所以伟大，就是通过获得的知识应用于实践，最后做了很大的生意。所以数据的应用分这四个步骤：数据，信息，知识，智慧。这是很多商家都想要的，你看我收集了这么多的数据，能不能基于这些数据来帮我做下一步的决策，改善我的产品，例如让用户看视频的时候旁边弹出广告，正好是他想买的东西，再如让用户听音乐的时候，另外推荐一些他非常想听的其他音乐。用户在我的应用或者网站上随便点点鼠标，输入文字对我来说都是数据，我就是要将其中某些东西提取出来，指导实践，形成智慧，让用户陷入到我的应用里面不可自拔，上了我的网就不想离开，手不停的点，不停的买，很多人说双十一我都想断网了，我老婆在上面不断的买买买，买了A又推荐B，老婆大人说，“哎呀，B也是我喜欢的啊，老公我要买”。你说这个程序怎么这么牛，这么有智慧，比我还了解我老婆，这件事情是怎么做到的呢？3.2 数据如何升华为智慧数据的处理分几个步骤，完成了才最后会有智慧。第一个步骤叫数据的收集。首先得有数据，数据的收集有两个方式，第一个方式是拿，专业点的说法叫抓取或者爬取，例如搜索引擎就是这么做的，它把网上的所有的信息都下载到它的数据中心，然后你一搜才能搜出来。比如你去搜索的时候，结果会是一个列表，这个列表为什么会在搜索引擎的公司里面呢，就是因为他把这个数据啊都拿下来了，但是你一点链接，点出来这个网站就不在搜索引擎它们公司了。比如说新浪有个新闻，你拿百度搜出来，你不点的时候，那一页在百度数据中心，一点出来的网页就是在新浪的数据中心了。另外一个方式就是推送，有很多终端可以帮我收集数据，比如说小米手环，可以将你每天跑步的数据，心跳的数据，睡眠的数据都上传到数据中心里面。第二个步骤是数据的传输。一般会通过队列方式进行，因为数据量实在是太大了，数据必须经过处理才会有用，可是系统处理不过来，只好排好队，慢慢的处理。第三个步骤是数据的存储。现在数据就是金钱，掌握了数据就相当于掌握了钱。要不然网站怎么知道你想买什么呢？就是因为它有你历史的交易的数据，这个信息可不能给别人，十分宝贵，所以需要存储下来。第四个步骤是数据的处理和分析。上面存储的数据是原始数据，原始数据多是杂乱无章的，有很多垃圾数据在里面，因而需要清洗和过滤，得到一些高质量的数据。对于高质量的数据，就可以进行分析，从而对数据进行分类，或者发现数据之间的相互关系，得到知识。比如盛传的沃尔玛超市的啤酒和尿布的故事，就是通过对人们的购买数据进行分析，发现了男人一般买尿布的时候，会同时购买啤酒，这样就发现了啤酒和尿布之间的相互关系，获得知识，然后应用到实践中，将啤酒和尿布的柜台弄的很近，就获得了智慧。第五个步骤就是对于数据的检索和挖掘。检索就是搜索，所谓外事不决问google，内事不决问百度。内外两大搜索引擎都是讲分析后的数据放入搜索引擎，从而人们想寻找信息的时候，一搜就有了。另外就是挖掘，仅仅搜索出来已经不能满足人们的要求了，还需要从信息中挖掘出相互的关系。比如财经搜索，当搜索某个公司股票的时候，该公司的高管是不是也应该被挖掘出来呢？如果仅仅搜索出这个公司的股票发现涨的特别好，于是你就去买了，其实其高管发了一个声明，对股票十分不利，第二天就跌了，这不坑害广大股民么？所以通过各种算法挖掘数据中的关系，形成知识库，十分重要。3.3 大数据时代，众人拾柴火焰高当数据量很小的时候，很少的几台机器就能解决。慢慢的当数据量越来越大，最牛的服务器都解决不了问题的时候，就想怎么办呢？要聚合多台机器的力量，大家齐心协力一起把这个事搞定，众人拾柴火焰高。对于数据的收集，对于IoT来讲，外面部署这成千上万的检测设备，将大量的温度，适度，监控，电力等等数据统统收集上来，对于互联网网页的搜索引擎来讲，需要将整个互联网所有的网页都下载下来，这显然一台机器做不到，需要多台机器组成网络爬虫系统，每台机器下载一部分，同时工作，才能在有限的时间内，将海量的网页下载完毕。对于数据的传输，一个内存里面的队列肯定会被大量的数据挤爆掉，于是就产生了基于硬盘的分布式队列，这样队列可以多台机器同时传输，随你数据量多大，只要我的队列足够多，管道足够粗，就能够撑得住。对于数据的存储，一台机器的文件系统肯定是放不下了，所以需要一个很大的分布式文件系统来做这件事情，把多台机器的硬盘打成一块大的文件系统。再如数据的分析，可能需要对大量的数据做分解，统计，汇总，一台机器肯定搞不定，处理到猴年马月也分析不完，于是就有分布式计算的方法，将大量的数据分成小份，每台机器处理一小份，多台机器并行处理，很快就能算完。例如著名的Terasort对1个TB的数据排序，相当于1000G，如果单机处理，怎么也要几个小时，但是并行处理209秒就完成了。所以说大数据平台，什么叫做大数据，说白了就是一台机器干不完，大家一起干。随着数据量越来越大，很多不大的公司都需要处理相当多的数据，这些小公司没有这么多机器可怎么办呢？3.4 大数据需要云计算，云计算需要大数据说到这里，大家想起云计算了吧。当想要干这些活的时候，需要好多好多的机器一块做，真的是想什么时候要，想要多少就要多少。例如大数据分析公司的财务情况，可能一周分析一次，如果要把这一百台机器或者一千台机器都在那放着，一周用一次对吧，非常浪费。那能不能需要计算的时候，把这一千台机器拿出来，然后不算的时候，这一千台机器可以去干别的事情。谁能做这个事儿呢？只有云计算，可以为大数据的运算提供资源层的灵活性。而云计算也会部署大数据放到它的PaaS平台上，作为一个非常非常重要的通用应用。因为大数据平台能够使得多台机器一起干一个事儿，这个东西不是一般人能开发出来的，也不是一般人玩得转的，怎么也得雇个几十上百号人才能把这个玩起来，所以说就像数据库一样，其实还是需要有一帮专业的人来玩这个东西。现在公有云上基本上都会有大数据的解决方案了，一个小公司我需要大数据平台的时候，不需要采购一千台机器，只要到公有云上一点，这一千台机器都出来了，并且上面已经部署好了的大数据平台，只要把数据放进去算就可以了。云计算需要大数据，大数据需要云计算，两个人就这样结合了。四、人工智能拥抱大数据4.1 机器什么时候才能懂人心虽说有了大数据，人的欲望总是这个不能够满足。虽说在大数据平台里面有搜索引擎这个东西，想要什么东西我一搜就出来了。但是也存在这样的情况，我想要的东西不会搜，表达不出来，搜索出来的又不是我想要的。例如音乐软件里面推荐一首歌，这首歌我没听过，当然不知道名字，也没法搜，但是软件推荐给我，我的确喜欢，这就是搜索做不到的事情。当人们使用这种应用的时候，会发现机器知道我想要什么，而不是说当我想要的时候，去机器里面搜索。这个机器真像我的朋友一样懂我，这就有点人工智能的意思了。人们很早就在想这个事情了。最早的时候，人们想象，如果要是有一堵墙，墙后面是个机器，我给它说话，它就给我回应，我如果感觉不出它那边是人还是机器，那它就真的是一个人工智能的东西了。4.2 让机器学会推理怎么才能做到这一点呢？人们就想：我首先要告诉计算机人类的推理的能力。你看人重要的是什么呀，人和动物的区别在什么呀，就是能推理。我要是把我这个推理的能力啊告诉机器，机器就能根据你的提问，推理出相应的回答，真能这样多好。推理其实人们慢慢的让机器能够做到一些了，例如证明数学公式。这是一个非常让人惊喜的一个过程，机器竟然能够证明数学公式。但是慢慢发现其实这个结果，也没有那么令人惊喜，因为大家发现了一个问题，数学公式非常严谨，推理过程也非常严谨，而且数学公式很容易拿机器来进行表达，程序也相对容易表达。然而人类的语言就没这么简单了，比如今天晚上，你和你女朋友约会，你女朋友说：如果你早来，我没来，你等着，如果我早来，你没来，你等着。这个机器就比比较难理解了，但是人都懂，所以你和女朋友约会，你是不敢迟到的。4.3 教给机器知识所以仅仅告诉机器严格的推理是不够的，还要告诉机器一些知识。但是知识这个事儿，一般人可能就做不来了，可能专家可以，比如语言领域的专家，或者财经领域的专家。语言领域和财经领域知识能不能表示成像数学公式一样稍微严格点呢？例如语言专家可能会总结出主谓宾定状补这些语法规则，主语后面一定是谓语，谓语后面一定是宾语，将这些总结出来，并严格表达出来不久行了吗？后来发现这个不行，太难总结了，语言表达千变万化。就拿主谓宾的例子，很多时候在口语里面就省略了谓语，别人问：你谁啊？我回答：我刘超。但是你不能规定在语音语义识别的时候，要求对着机器说标准的书面语，这样还是不够智能，就像罗永浩在一次演讲中说的那样，每次对着手机，用书面语说：请帮我呼叫某某某，这是一件很尴尬的事情。人工智能这个阶段叫做专家系统。专家系统不易成功，一方面是知识比较难总结，另一方面总结出来的知识难以教给计算机。因为你自己还迷迷糊糊，似乎觉得有规律，就是说不出来，就怎么能够通过编程教给计算机呢？4.4 算了，教不会你自己学吧于是人们想到，看来机器是和人完全不一样的物种，干脆让机器自己学习好了。机器怎么学习呢？既然机器的统计能力这么强，基于统计学习，一定能从大量的数字中发现一定的规律。其实在娱乐圈有很好的一个例子，可见一斑有一位网友统计了知名歌手在大陆发行的 9 张专辑中 117 首歌曲的歌词，同一词语在一首歌出现只算一次，形容词、名词和动词的前十名如下表所示（词语后面的数字是出现的次数）：如果我们随便写一串数字，然后按照数位依次在形容词、名词和动词中取出一个词，连在一起会怎么样呢？例如取圆周率 3.1415926，对应的词语是：坚强，路，飞，自由，雨，埋，迷惘。稍微连接和润色一下：坚强的孩子，依然前行在路上，张开翅膀飞向自由，让雨水埋葬他的迷惘。是不是有点感觉了？当然真正基于统计的学习算法比这个简单的统计复杂的多。然而统计学习比较容易理解简单的相关性，例如一个词和另一个词总是一起出现，两个词应该有关系，而无法表达复杂的相关性，并且统计方法的公式往往非常复杂，为了简化计算，常常做出各种独立性的假设，来降低公式的计算难度，然而现实生活中，具有独立性的事件是相对较少的。4.5 模拟大脑的工作方式于是人类开始从机器的世界，反思人类的世界是怎么工作的。人类的脑子里面不是存储着大量的规则，也不是记录着大量的统计数据，而是通过神经元的触发实现的，每个神经元有从其他神经元的输入，当接收到输入的时候，会产生一个输出来刺激其他的神经元，于是大量的神经元相互反应，最终形成各种输出的结果。例如当人们看到美女瞳孔放大，绝不是大脑根据身材比例进行规则判断，也不是将人生中看过的所有的美女都统计一遍，而是神经元从视网膜触发到大脑再回到瞳孔。在这个过程中，其实很难总结出每个神经元对最终的结果起到了哪些作用，反正就是起作用了。于是人们开始用一个数学单元模拟神经元这个神经元有输入，有输出，输入和输出之间通过一个公式来表示，输入根据重要程度不同(权重)，影响着输出。于是将n个神经元通过像一张神经网络一样连接在一起，n这个数字可以很大很大，所有的神经元可以分成很多列，每一列很多个排列起来，每个神经元的对于输入的权重可以都不相同，从而每个神经元的公式也不相同。当人们从这张网络中输入一个东西的时候，希望输出一个对人类来讲正确的结果。例如上面的例子，输入一个写着2的图片，输出的列表里面第二个数字最大，其实从机器来讲，它既不知道输入的这个图片写的是2，也不知道输出的这一系列数字的意义，没关系，人知道意义就可以了。正如对于神经元来说，他们既不知道视网膜看到的是美女，也不知道瞳孔放大是为了看的清楚，反正看到美女，瞳孔放大了，就可以了。对于任何一张神经网络，谁也不敢保证输入是2，输出一定是第二个数字最大，要保证这个结果，需要训练和学习。毕竟看到美女而瞳孔放大也是人类很多年进化的结果。学习的过程就是，输入大量的图片，如果结果不是想要的结果，则进行调整。如何调整呢，就是每个神经元的每个权重都向目标进行微调，由于神经元和权重实在是太多了，所以整张网络产生的结果很难表现出非此即彼的结果，而是向着结果微微的进步，最终能够达到目标结果。当然这些调整的策略还是非常有技巧的，需要算法的高手来仔细的调整。正如人类见到美女，瞳孔一开始没有放大到能看清楚，于是美女跟别人跑了，下次学习的结果是瞳孔放大一点点，而不是放大鼻孔。4.6 没道理但做得到听起来也没有那么有道理，但是的确能做到，就是这么任性。神经网络的普遍性定理是这样说的，假设某个人给你某种复杂奇特的函数，f(x)：不管这个函数是什么样的，总会确保有个神经网络能够对任何可能的输入x，其值f(x)（或者某个能够准确的近似）是神经网络的输出。如果在函数代表着规律，也意味着这个规律无论多么奇妙，多么不能理解，都是能通过大量的神经元，通过大量权重的调整，表示出来的。4.7 人工智能的经济学解释这让我想到了经济学，于是比较容易理解了。我们把每个神经元当成社会中从事经济活动的个体。于是神经网络相当于整个经济社会，每个神经元对于社会的输入，都有权重的调整，做出相应的输出，比如工资涨了，菜价也涨了，股票跌了，我应该怎么办，怎么花自己的钱。这里面没有规律么？肯定有，但是具体什么规律呢？却很难说清楚。基于专家系统的经济属于计划经济，整个经济规律的表示不希望通过每个经济个体的独立决策表现出来，而是希望通过专家的高屋建瓴和远见卓识总结出来。专家永远不可能知道哪个城市的哪个街道缺少一个卖甜豆腐脑的。于是专家说应该产多少钢铁，产多少馒头，往往距离人民生活的真正需求有较大的差距，就算整个计划书写个几百页，也无法表达隐藏在人民生活中的小规律。基于统计的宏观调控就靠谱的多了，每年统计局都会统计整个社会的就业率，通胀率，GDP等等指标，这些指标往往代表着很多的内在规律，虽然不能够精确表达，但是相对靠谱。然而基于统计的规律总结表达相对比较粗糙，比如经济学家看到这些统计数据可以总结出长期来看房价是涨还是跌，股票长期来看是涨还是跌，如果经济总体上扬，房价和股票应该都是涨的。但是基于统计数据，无法总结出股票，物价的微小波动规律。基于神经网络的微观经济学才是对整个经济规律最最准确的表达，每个人对于从社会中的输入，进行各自的调整，并且调整同样会作为输入反馈到社会中。想象一下股市行情细微的波动曲线，正是每个独立的个体各自不断交易的结果，没有统一的规律可循。而每个人根据整个社会的输入进行独立决策，当某些因素经过多次训练，也会形成宏观上的统计性的规律，这也就是宏观经济学所能看到的。例如每次货币大量发行，最后房价都会上涨，多次训练后，人们也就都学会了。4.8 人工智能需要大数据然而神经网络包含这么多的节点，每个节点包含非常多的参数，整个参数量实在是太大了，需要的计算量实在太大，但是没有关系啊，我们有大数据平台，可以汇聚多台机器的力量一起来计算，才能在有限的时间内得到想要的结果。人工智能可以做的事情非常多，例如可以鉴别垃圾邮件，鉴别黄色暴力文字和图片等。这也是经历了三个阶段的。第一个阶段依赖于关键词黑白名单和过滤技术，包含哪些词就是黄色或者暴力的文字。随着这个网络语言越来越多，词也不断的变化，不断的更新这个词库就有点顾不过来。第二个阶段时，基于一些新的算法，比如说贝叶斯过滤等，你不用管贝叶斯算法是什么，但是这个名字你应该听过，这个一个基于概率的算法。第三个阶段就是基于大数据和人工智能，进行更加精准的用户画像和文本理解和图像理解。由于人工智能算法多是依赖于大量的数据的，这些数据往往需要面向某个特定的领域(例如电商，邮箱)进行长期的积累，如果没有数据，就算有人工智能算法也白搭，所以人工智能程序很少像前面的IaaS和PaaS一样，将人工智能程序给某个客户安装一套让客户去用，因为给某个客户单独安装一套，客户没有相关的数据做训练，结果往往是很差的。但是云计算厂商往往是积累了大量数据的，于是就在云计算厂商里面安装一套，暴露一个服务接口，比如您想鉴别一个文本是不是涉及黄色和暴力，直接用这个在线服务就可以了。这种形势的服务，在云计算里面称为软件即服务，SaaS (Software AS A Service)于是工智能程序作为SaaS平台进入了云计算。五、云计算，大数据，人工智能过上了美好的生活终于云计算的三兄弟凑齐了，分别是IaaS，PaaS和SaaS，所以一般在一个云计算平台上，云，大数据，人工智能都能找得到。对一个大数据公司，积累了大量的数据，也会使用一些人工智能的算法提供一些服务。对于一个人工智能公司，也不可能没有大数据平台支撑。所以云计算，大数据，人工智能就这样整合起来，完成了相遇，相识，相知。", "author": "刘超（popsuper1982）", "tags": ["IT技术", "云计算", "人工智能", "大数据"]}
{"url_object_id": "957e4ea5a4a89c6e0be1752b3db3a295", "title": "10 个常用的软件架构模式", "url": "http://blog.jobbole.com/113953/", "create_date": "2018/05/13", "praise_nums": "2", "favor_nums": 10, "comment_nums": 0, "content": "你是否曾经思考过如何设计大型的企业级系统？在决定启动软件开发之前，首要的是选择恰当的架构来指引系统的功能及质量属性设计。因此在将软件架构应用于设计之前，必需要了解常用的架构模式。Wikipedia 的解释：这篇文章将简述常见的 10 种架构模式的概念、用法以及其优缺点。分层模式用于对结构化设计的软件进行层次拆解，每个层次为独立的抽象，为其上层抽象提供服务。系统通常被拆分为以下四个层次：表示层（也称为 UI 层）应用层（也称为服务层）业务逻辑层（也称为领域层）数据访问层（也称为持久化层）使用场景通用桌面应用程序电子商务 Web 应用客户端／服务器模式由两个部分构成：一个服务器与多个客户端。服务器组件同时为多个客户端组件提供服务。客户端向服务器发启服务请求，服务器将相应服务信息回应给客户端。此外，服务器持续监听来自客户端的请求。使用场景电子邮件、文件共享及银行业务等在线应用主／从模式由两个部分构成：主设备与从设备。主服务组件将作业分发给多个从设备组件，并根据这些从设备反馈的结果，计算生成最终结果。使用场景数据库复制，主数据库被认定为权威数据源，各从数据库与主数据保持同步在计算机系统中通过总线互连的各设备（包括主设备与从设备）管道／过滤器模式用于构造用于生成及处理数据流的系统。每个处理过程都封装在过滤器（filter）组件之中，要处理的数据通过 管道(pips) 进行投递。管道同时用于作为 过滤器（filter） 间的缓冲及同步。使用场景编译器，一系列的过滤器用于词法分析、语法分析、语义分析及代码生成生物信息学的工作流代理模式用于在结构化系统中对组件解耦。系统内各组件间采用远过程调用（remote service invocations）的方式交互。代理（Broker）组件充当组件间通讯的协调角色。提供服务的组件将其能力（服务以及特性）发布给代理，客户端均向代理请求服务，由代理将请求重定向到先前已发布过对应服务的组件进行处理。消息中间件软件：Apache ActiveMQ，Apache Kafka，RabbitMQ 与 JBoss 等等对等模式中的组件称之为对等体（peer），对等体既作为向其他对等体请求服务的客户端，同时也做为响应其他对等体请求的服务端。对等体可以在运行过程中动态地改变其角色，即，既可以单独做为客户端或服务端运行，又可同时作为客户端与服务端运行。使用场景网络文件共享： 与 )流媒体协议： 与 .流媒体应用： .事件总线模式应用于事件处理，主要由四个组件构成：事件源（event source），事件侦听者（event listener），通道（Channel）以及总线（event bus）。 事件源将消息发布到总线的特定通道，侦听者订阅相应的通道，事件源所发布的消息经通道通告给订阅通道的侦听者。使用场景Android 开发通告（Notification）服务模型／视图／控制器模式（简称 MVC 模式）将交互式应用程序拆分为三个部分：MVC 模式通过将内部信息表示、用户信息呈现以及用户操作接收分开的方式解耦组件，实现高效代码重用。使用场景主流开发语言所构建的互联网网页应用架构 与  等网页应用开发框架黑板模式适用于 无预知确定解决策略 的问题，主要由三个组件构成： – 用于存储解空间对象的结构化全局内存源 – 能自表意的专用模块组件 – 选择、配置与执行的模块所有的组件均能访问黑板，组件可将新生成的数据对象写入黑板，也可以通过模式匹配从黑板中获取知识源所生成的特定数据。使用场景语音识别车辆识别和追踪蛋白质的结构鉴定声纳信号解析解析器模式用于设计语言的解析程序，主要用于指定评估程序代码行，即解析出特定语言的语句与表达式，其核心思想是为语言的每个符号定义相应的类。使用场景SQL 等数据库查询语言通讯协议描述语言下表格总结了各架构模式的优缺点希望这篇文章对你所帮助，同时我也想听听你的想法。😇感谢阅读 ", "author": "伯乐在线", "tags": ["IT技术"]}
{"url_object_id": "06b67f2d6e9b1c6772e9dc3273786a32", "title": "分布式之缓存击穿", "url": "http://blog.jobbole.com/114012/", "create_date": "2018/05/19", "praise_nums": "2", "favor_nums": 1, "comment_nums": 0, "content": "在谈论缓存击穿之前，我们先来回忆下从缓存中加载数据的逻辑，如下图所示\n因此，如果黑客每次故意查询一个在缓存内必然不存在的数据，导致每次请求都要去存储层去查询，这样缓存就失去了意义。如果在大流量下数据库可能挂掉。这就是缓存击穿。\n场景如下图所示:\n我们正常人在登录首页的时候，都是根据userID来命中数据，然而黑客的目的是破坏你的系统，黑客可以随机生成一堆userID,然后将这些请求怼到你的服务器上，这些请求在缓存中不存在，就会穿过缓存，直接怼到数据库上,从而造成数据库连接异常。在这里我们给出三套解决方案，大家根据项目中的实际情况，选择使用.讲下述三种方案前，我们先回忆下redis的setnx方法SETNX key value将 key 的值设为 value ，当且仅当 key 不存在。若给定的 key 已经存在，则 SETNX 不做任何动作。SETNX 是『SET if Not eXists』(如果不存在，则 SET)的简写。可用版本：>= 1.0.0时间复杂度： O(1)返回值： 设置成功，返回 1。设置失败，返回 0 。效果如下redis> EXISTS job                # job 不存在(integer) 1redis> SETNX job \"code-farmer\"   # 尝试覆盖 job ，失败\"programmer\"1、使用互斥锁该方法是比较普遍的做法，即，在根据key获得的value值为空时，先锁上，再从数据库加载，加载完毕，释放锁。若其他线程发现获取锁失败，则睡眠50ms后重试。至于锁的类型，单机环境用并发包的Lock类型就行，集群环境则使用分布式锁( redis的setnx)集群环境的redis的代码如下所示:String get(String key) {     if (value  == null) {          // 3 min timeout to avoid mutex holder crash          value = db.get(key);          redis.delete(key_mutex);          //其他线程休息50毫秒后重试          get(key);    }  优点:缺点2、异步构建缓存在这种方案下，构建缓存采取异步策略，会从线程池中取线程来异步构建缓存，从而不会让所有的请求直接怼到数据库上。该方案redis自己维护一个timeout，当timeout小于System.currentTimeMillis()时，则进行缓存更新，否则直接返回value值。\n集群环境的redis代码如下所示:String get(final String key) {          String value = v.getValue();          if (v.timeout <= System.currentTimeMillis()) {              threadPool.execute(new Runnable() {                      String keyMutex = \"mutex:\" + key;                          // 3 min timeout to avoid mutex holder crash                          String dbValue = db.get(key);                          redis.delete(keyMutex);                  }          }      }优点:缺点3、布隆过滤器布隆过滤器的巨大用处就是，能够迅速判断一个元素是否在一个集合中。因此他有如下三个使用场景:OK，接下来我们来谈谈布隆过滤器的原理\n其内部维护一个全为0的bit数组，需要说明的是，布隆过滤器有一个误判率的概念，误判率越低，则数组越长，所占空间越大。误判率越高则数组越小，所占的空间越小。假设，根据误判率，我们生成一个10位的bit数组，以及2个hash函数（(f_1,f_2)），如下图所示(生成的数组的位数和hash函数的数量，我们不用去关心是如何生成的，有数学论文进行过专业的证明)。\n假设输入集合为((N_1,N_2)),经过计算(f_1(N_1))得到的数值得为2，(f_2(N_1))得到的数值为5，则将数组下标为2和下表为5的位置置为1，如下图所示\n同理，经过计算(f_1(N_2))得到的数值得为3，(f_2(N_2))得到的数值为6，则将数组下标为3和下表为6的位置置为1，如下图所示\n这个时候，我们有第三个数(N_3)，我们判断(N_3)在不在集合((N_1,N_2))中，就进行(f_1(N_3)，f_2(N_3))的计算以上就是布隆过滤器的计算原理，下面我们进行性能测试，代码如下:<dependencies>              <groupId>com.google.guava</groupId>              <version>22.0</version>      </dependencies>package bloomfilter;import com.google.common.hash.BloomFilter;import java.nio.charset.Charset;public class Test {        for (int i = 0; i < size; i++) {        }                if (bloomFilter.mightContain(29999)) {        }}输出如下所示也就是说，判断一个数是否属于一个百万级别的集合，只要0.219ms就可以完成，性能极佳。首先，我们先不对误判率做显示的设置，进行一个测试，代码如下所示package bloomfilter;import java.util.ArrayList;import com.google.common.hash.Funnels;public class Test {        for (int i = 0; i < size; i++) {        }                for (int i = size + 10000; i < size + 20000; i++) {                  list.add(i);          }  }输出结果如下330如果上述代码所示，我们故意取10000个不在过滤器里的值，却还有330个被认为在过滤器里，这说明了误判率为0.03.即，在不做任何设置的情况下，默认的误判率为0.03。\n下面上源码来证明：\n接下来我们来看一下，误判率为0.03时，底层维护的bit数组的长度如下图所示\n将bloomfilter的构造方法改为private static BloomFilter<Integer> bloomFilter = BloomFilter.create(Funnels.integerFunnel(), size,0.01);即，此时误判率为0.01。在这种情况下，底层维护的bit数组的长度如下图所示\n\n由此可见，误判率越低，则底层维护的数组越长，占用空间越大。因此，误判率实际取值，根据服务器所能够承受的负载来决定，不是拍脑袋瞎想的。redis伪代码如下所示String get(String key) {     if (value  == null) {              return null;           value = db.get(key);          }    return value优点:缺点在总结部分，来个漫画把。希望对大家找工作有帮助\n", "author": "孤独烟", "tags": ["IT技术", "分布式", "数据库"]}
{"url_object_id": "50e131c5200d426df175c11073a3dcc7", "title": "如何在 Linux 中使用 find", "url": "http://blog.jobbole.com/114000/", "create_date": "2018/05/17", "praise_nums": "1", "favor_nums": 2, "comment_nums": 0, "content": "在最近的一篇文章中，Lewis Cowles 介绍了 find 命令。find 是日常工具箱中功能更强大、更灵活的命令行工具之一，因此值得花费更多的时间。最简单的，find 跟上路径寻找一些东西。例如：find /它将找到（并打印出）系统中的每个文件。而且由于一切都是文件，你会得到很多需要整理的输出。这可能不能帮助你找到你要找的东西。你可以改变路径参数来缩小范围，但它不会比使用 ls 命令更有帮助。所以你需要考虑你想要找的东西。也许你想在主目录中找到所有的 JPEG 文件。 -name 参数允许你将结果限制为与给定模式匹配的文件。find ~ -name '*jpg'可是等等！如果它们中的一些是大写的扩展名会怎么样？-iname 就像 -name，但是不区分大小写。find ~ -iname '*jpg'很好！但是 8.3 名称方案是如此的老。一些图片可能是 .jpeg 扩展名。幸运的是，我们可以将模式用“或”（表示为 -o）来组合。find ~ ( -iname 'jpeg' -o -iname 'jpg' )我们正在接近目标。但是如果你有一些以 jpg 结尾的目录呢？ （为什么你要命名一个 bucketofjpg 而不是 pictures 的目录就超出了本文的范围。）我们使用 -type 参数修改我们的命令来查找文件。find ~ \\( -iname '*jpeg' -o -iname '*jpg' \\) -type f或者，也许你想找到那些命名奇怪的目录，以便稍后重命名它们：find ~ \\( -iname '*jpeg' -o -iname '*jpg' \\) -type d你最近拍了很多照片，所以让我们把它缩小到上周更改的文件。find ~ \\( -iname '*jpeg' -o -iname '*jpg' \\) -type f -mtime -7你可以根据文件状态更改时间 （ctime）、修改时间 （mtime） 或访问时间 （atime） 来执行时间过滤。 这些是在几天内，所以如果你想要更细粒度的控制，你可以表示为在几分钟内（分别是 cmin、mmin 和 amin）。 除非你确切地知道你想要的时间，否则你可能会在 + （大于）或 - （小于）的后面加上数字。但也许你不关心你的照片。也许你的磁盘空间不够用，所以你想在 log 目录下找到所有巨大的（让我们定义为“大于 1GB”）文件：find /var/log -size +1G或者，也许你想在 /data 中找到 bcotton 拥有的所有文件：find /data -owner bcotton你还可以根据权限查找文件。也许你想在你的主目录中找到对所有人可读的文件，以确保你不会过度分享。find ~ -perm -o=r这篇文章只说了 find 能做什么的表面。将测试条件与布尔逻辑相结合可以为你提供难以置信的灵活性，以便准确找到要查找的文件。并且像 -exec 或 -delete 这样的参数，你可以让 find 对它发现的内容采取行动。你有任何最喜欢的 find 表达式么？在评论中分享它们！ ", "author": "Ben Cotton", "tags": ["IT技术", "Linux"]}
{"url_object_id": "41201d7935af5be0d5b31cfa94a23f85", "title": "JAVA 程序员需要用到 10 个测试框架和库", "url": "http://blog.jobbole.com/113952/", "create_date": "2018/05/14", "praise_nums": "1", "favor_nums": 0, "comment_nums": 0, "content": "", "author": "伯乐在线", "tags": ["工具与资源", "Groovy", "java", "JUnit"]}
{"url_object_id": "6dc6e928199fafa86ce672b2bf1253e2", "title": "给初学者看的 shuf 命令教程", "url": "http://blog.jobbole.com/113985/", "create_date": "2018/05/14", "praise_nums": "1", "favor_nums": 0, "comment_nums": 0, "content": "shuf 命令用于在类 Unix 操作系统中生成随机排列。使用 shuf 命令，我们可以随机打乱给定输入文件的行。shuf 命令是 GNU Coreutils 的一部分，因此你不必担心安装问题。在这个简短的教程中，让我向你展示一些 shuf 命令的例子。$ cat ostechnix.txtline2line4line6line8line10$ shuf ostechnix.txtline8line10line1line6line3$ touch output.txt$ shuf ostechnix.txt -o output.txt$ cat output.txtline2line9line1line7line4$ shuf -n 1 ostechnix.txt$ shuf -n 5 ostechnix.txtline4line9$ shuf -e line1 line2 line3 line4 line5line3line4$ shuf -e 1 2 3 4 554$ shuf -n 1 -e 1 2 3 4 5$ shuf -n 3 -e 1 2 3 4 55$ shuf -i 1-1092735$ man shuf", "author": "Sk", "tags": ["IT技术", "Linux"]}
{"url_object_id": "b56537dc0b776f7afb12d3ecdd87b33c", "title": "常用排序算法总结（2）", "url": "http://blog.jobbole.com/113977/", "create_date": "2018/05/13", "praise_nums": "1", "favor_nums": 2, "comment_nums": 0, "content": "上一篇总结了常用的比较排序算法，主要有冒泡排序，选择排序，插入排序，归并排序，堆排序，快速排序等。这篇文章中我们来探讨一下常用的非比较排序算法：计数排序，基数排序，桶排序。在一定条件下，它们的时间复杂度可以达到O(n)。这里我们用到的唯一数据结构就是数组，当然我们也可以利用链表来实现下述算法。计数排序用到一个额外的计数数组C，根据数组C来将原数组A中的元素排到正确的位置。通俗地理解，例如有10个年龄不同的人，假如统计出有8个人的年龄不比小明大（即小于等于小明的年龄，这里也包括了小明），那么小明的年龄就排在第8位，通过这种思想可以确定每个人的位置，也就排好了序。当然，年龄一样时需要特殊处理（保证稳定性）：通过反向填充目标数组，填充完毕后将对应的数字统计递减，可以确保计数排序的稳定性。计数排序的步骤如下：计数排序的实现代码如下：#include<iostream>// 数据结构 --------- 数组// 最优时间复杂度 ---- O(n + k)// 所需辅助空间 ------ O(n + k)const int k = 100;   // 基数为100，排序[0,99]内的整数{    {    }    {    }    {    }    for (int i = n - 1; i >= 0; i--)    // 从后向前扫描保证计数排序的稳定性(重复元素相对次序不变)        B[--C[A[i]]] = A[i];      // 把每个元素A[i]放到它在输出数组B中的正确位置上    }    {    }}int main()    int A[] = { 15, 22, 19, 46, 27, 73, 1, 19, 8 };  // 针对计数排序设计的输入，每一个元素都在[0,100]上且有重复元素    CountingSort(A, n);    for (int i = 0; i < n; i++)        printf(\"%d \", A[i]);    printf(\"\\n\");}下图给出了对{ 4, 1, 3, 4, 3 }进行计数排序的简单演示过程计数排序的时间复杂度和空间复杂度与数组A的数据范围（A中元素的最大值与最小值的差加上1）有关，因此对于数据范围很大的数组，计数排序需要大量时间和内存。例如：对0到99之间的数字进行排序，计数排序是最好的算法，然而计数排序并不适合按字母顺序排序人名，将计数排序用在基数排序算法中，能够更有效的排序数据范围很大的数组。基数排序的发明可以追溯到1887年赫尔曼·何乐礼在打孔卡片制表机上的贡献。它是这样实现的：将所有待比较正整数统一为同样的数位长度，数位较短的数前面补零。然后，从最低位开始进行基数为10的计数排序，一直到最高位计数排序完后，数列就变成一个有序序列（利用了计数排序的稳定性）。基数排序的实现代码如下：#include<iostream>// 数据结构 ---------- 数组// 最优时间复杂度 ---- O(n * dn)// 所需辅助空间 ------ O(n * dn)const int k = 10;                // 基数为10，每一位的数字都是[0,9]内的整数{    return (x / radix[d]) % 10;{    {    }    {    }    {    }    for (int i = n - 1; i >= 0; i--)        int dight = GetDigit(A[i], d);  // 元素A[i]当前位数字为dight           // 当再遇到当前位数字同为dight的元素时，会将其放在当前元素的前一个位置上保证计数排序的稳定性    for (int i = 0; i < n; i++)        A[i] = B[i];    free(B);{        CountingSort(A, n, d);        // 依据第d位数字对A进行计数排序{    int n = sizeof(A) / sizeof(int);    printf(\"基数排序结果：\");    {    }    return 0;下图给出了对{ 329, 457, 657, 839, 436, 720, 355 }进行基数排序的简单演示过程基数排序的时间复杂度是O(n * dn)，其中n是待排序元素个数，dn是数字位数。这个时间复杂度不一定优于O(n log n)，dn的大小取决于数字位的选择（比如比特位数），和待排序数据所属数据类型的全集的大小；dn决定了进行多少轮处理，而n是每轮处理的操作数目。如果考虑和比较排序进行对照，基数排序的形式复杂度虽然不一定更小，但由于不进行比较，因此其基本操作的代价较小，而且如果适当的选择基数，dn一般不大于log n，所以基数排序一般要快过基于比较的排序，比如快速排序。由于整数也可以表达字符串（比如名字或日期）和特定格式的浮点数，所以基数排序并不是只能用于整数排序。桶排序也叫箱排序。工作的原理是将数组元素映射到有限数量个桶里，利用计数排序可以定位桶的边界，每个桶再各自进行桶内排序（使用其它排序算法或以递归方式继续使用桶排序）。桶排序的实现代码如下：#include<iostream>// 数据结构 --------- 数组// 最优时间复杂度 ---- O(n)，每个元素占一个桶// 所需辅助空间 ------ O(n + bn)const int bn = 5;    // 这里排序[0,49]的元素，使用5个桶就够了，也可以根据输入动态确定桶的数量{    {        int j = i - 1;        {            j--;        A[j + 1] = get;}int MapToBucket(int x)    return x / 10;    // 映射函数f(x)，作用相当于快排中的Partition，把大量数据分割成基本有序的数据块{    {    }    {    }    {    }    for (int i = n - 1; i >= 0; i--)// 从后向前扫描保证计数排序的稳定性(重复元素相对次序不变)        int b = MapToBucket(A[i]);  // 元素A[i]位于b号桶                                    // 桶的边界被更新：C[b]为b号桶第一个元素的位置    for (int i = 0; i < n; i++)        A[i] = B[i];    free(B);{    for (int i = 0; i < bn; i++) // 对每一个桶中的元素应用插入排序        int left = C[i];         // C[i]为i号桶第一个元素的位置        if (left < right)        // 对元素个数大于1的桶进行桶内插入排序    }{    int n = sizeof(A) / sizeof(int);    printf(\"桶排序结果：\");    {    }    return 0;下图给出了对{ 29, 25, 3, 49, 9, 37, 21, 43 }进行桶排序的简单演示过程桶排序不是比较排序，不受到O(nlogn)下限的影响，它是鸽巢排序的一种归纳结果，当所要排序的数组值分散均匀的时候，桶排序拥有线性的时间复杂度。", "author": "SteveWang", "tags": ["IT技术", "算法"]}
{"url_object_id": "57606b7f23e9680d531bf21cd6736b58", "title": "大龄码农的新西兰移民之路", "url": "http://blog.jobbole.com/113987/", "create_date": "2018/05/17", "praise_nums": "1", "favor_nums": 6, "comment_nums": 1, "content": "", "author": "yan_xiaodi（微信公众号：nzcoder ）", "tags": ["在国外", " 1 评论 ", "新西兰", "移民"]}
{"url_object_id": "10ced989709fd8ca75a2272ea7f16567", "title": "常用排序算法总结（1）", "url": "http://blog.jobbole.com/113863/", "create_date": "2018/05/11", "praise_nums": "2", "favor_nums": 5, "comment_nums": 0, "content": "我们通常所说的排序算法往往指的是内部排序算法，即数据记录在内存中进行排序。排序算法大体可分为两种：一种是比较排序，时间复杂度O(nlogn) ~ O(n^2)，主要有：冒泡排序，选择排序，插入排序，归并排序，堆排序，快速排序等。另一种是非比较排序，时间复杂度可以达到O(n)，主要有：计数排序，基数排序，桶排序等。这里我们来探讨一下常用的比较排序算法，非比较排序算法将在下一篇文章中介绍。下表给出了常见比较排序算法的性能：有一点我们很容易忽略的是排序算法的稳定性(腾讯校招2016笔试题曾考过)。排序算法稳定性的简单形式化定义为：如果A = A，排序前A在A之前，排序后A还在A之前，则称这种排序算法是稳定的。通俗地讲就是保证排序前后两个相等的数的相对顺序不变。对于不稳定的排序算法，只要举出一个实例，即可说明它的不稳定性；而对于稳定的排序算法，必须对算法进行分析从而得到稳定的特性。需要注意的是，排序算法是否为稳定的是由具体算法决定的，不稳定的算法在某种条件下可以变为稳定的算法，而稳定的算法在某种条件下也可以变为不稳定的算法。例如，对于冒泡排序，原本是稳定的排序算法，如果将记录交换的条件改成A[i] >= A[i + 1]，则两个相等的记录就会交换位置，从而变成不稳定的排序算法。其次，说一下排序算法稳定性的好处。排序算法如果是稳定的，那么从一个键上排序，然后再从另一个键上排序，前一个键排序的结果可以为后一个键排序所用。基数排序就是这样，先按低位排序，逐次按高位排序，低位排序后元素的顺序在高位也相同时是不会改变的。冒泡排序是一种极其简单的排序算法，也是我所学的第一个排序算法。它重复地走访过要排序的元素，依次比较相邻两个元素，如果他们的顺序错误就把他们调换过来，直到没有元素再需要交换，排序完成。这个算法的名字由来是因为越小(或越大)的元素会经由交换慢慢“浮”到数列的顶端。冒泡排序算法的运作如下：由于它的简洁，冒泡排序通常被用来对于程序设计入门的学生介绍算法的概念。冒泡排序的代码如下：#include <stdio.h>// 分类 -------------- 内部比较排序// 最差时间复杂度 ---- O(n^2)// 平均时间复杂度 ---- O(n^2)// 稳定性 ------------ 稳定void Swap(int A[], int i, int j)    int temp = A[i];    A[j] = temp;{    {        {            {            }    }{    int n = sizeof(A) / sizeof(int);    printf(\"冒泡排序结果：\");    {    }    return 0;上述代码对序列{ 6, 5, 3, 1, 8, 7, 2, 4 }进行冒泡排序的实现过程如下使用冒泡排序为一列数字进行排序的过程如右图所示：尽管冒泡排序是最容易了解和实现的排序算法之一，但它对于少数元素之外的数列排序是很没有效率的。冒泡排序的改进：鸡尾酒排序鸡尾酒排序，也叫定向冒泡排序，是冒泡排序的一种改进。此算法与冒泡排序的不同处在于从低到高然后从高到低，而冒泡排序则仅从低到高去比较序列里的每个元素。他可以得到比冒泡排序稍微好一点的效能。鸡尾酒排序的代码如下：#include <stdio.h>// 分类 -------------- 内部比较排序// 最差时间复杂度 ---- O(n^2)// 平均时间复杂度 ---- O(n^2)// 稳定性 ------------ 稳定void Swap(int A[], int i, int j)    int temp = A[i];    A[j] = temp;{    int right = n - 1;    {        {            {            }        right--;        {            {            }        left++;}int main()    int A[] = { 6, 5, 3, 1, 8, 7, 2, 4 };   // 从小到大定向冒泡排序    CocktailSort(A, n);    for (int i = 0; i < n; i++)        printf(\"%d \", A[i]);    printf(\"\\n\");}使用鸡尾酒排序为一列数字进行排序的过程如右图所示：以序列(2,3,4,5,1)为例，鸡尾酒排序只需要访问一次序列就可以完成排序，但如果使用冒泡排序则需要四次。但是在乱数序列的状态下，鸡尾酒排序与冒泡排序的效率都很差劲。　　选择排序也是一种简单直观的排序算法。它的工作原理很容易理解：初始时在序列中找到最小（大）元素，放到序列的起始位置作为已排序序列；然后，再从剩余未排序元素中继续寻找最小（大）元素，放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。注意选择排序与冒泡排序的区别：冒泡排序通过依次交换相邻两个顺序不合法的元素位置，从而将当前最小（大）元素放到合适的位置；而选择排序每遍历一次都记住了当前最小（大）元素的位置，最后仅需一次交换操作即可将其放到合适的位置。选择排序的代码如下：#include <stdio.h>// 分类 -------------- 内部比较排序// 最差时间复杂度 ---- O(n^2)// 平均时间复杂度 ---- O(n^2)// 稳定性 ------------ 不稳定void Swap(int A[], int i, int j)    int temp = A[i];    A[j] = temp;{    {        for (int j = i + 1; j < n; j++)     // 未排序序列            if (A[j] < A[min])              // 找出未排序序列中的最小值                min = j;        }        {        }}int main()    int A[] = { 8, 5, 2, 6, 9, 3, 1, 4, 0, 7 }; // 从小到大选择排序    SelectionSort(A, n);    for (int i = 0; i < n; i++)        printf(\"%d \", A[i]);    printf(\"\\n\");}上述代码对序列{ 8, 5, 2, 6, 9, 3, 1, 4, 0, 7 }进行选择排序的实现过程如右图：使用选择排序为一列数字进行排序的宏观过程：选择排序是不稳定的排序算法，不稳定发生在最小元素与A[i]交换的时刻。比如序列：{ 5, 8, 5, 2, 9 }，一次选择的最小元素是2，然后把2和第一个5进行交换，从而改变了两个元素5的相对次序。插入排序是一种简单直观的排序算法。它的工作原理非常类似于我们抓扑克牌对于未排序数据(右手抓到的牌)，在已排序序列(左手已经排好序的手牌)中从后向前扫描，找到相应位置并插入。插入排序在实现上，通常采用in-place排序（即只需用到O(1)的额外空间的排序），因而在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。具体算法描述如下：插入排序的代码如下：#include <stdio.h>// 分类 ------------- 内部比较排序// 最差时间复杂度 ---- 最坏情况为输入序列是降序排列的,此时时间复杂度O(n^2)// 平均时间复杂度 ---- O(n^2)// 稳定性 ------------ 稳定void InsertionSort(int A[], int n)    for (int i = 1; i < n; i++)         // 类似抓扑克牌排序        int get = A[i];                 // 右手抓到一张扑克牌        while (j >= 0 && A[j] > get)    // 将抓到的牌与手牌从右向左进行比较            A[j + 1] = A[j];            // 如果该手牌比抓到的牌大，就将其右移        }    }{    int n = sizeof(A) / sizeof(int);    printf(\"插入排序结果：\");    {    }    return 0;上述代码对序列{ 6, 5, 3, 1, 8, 7, 2, 4 }进行插入排序的实现过程如下使用插入排序为一列数字进行排序的宏观过程：插入排序不适合对于数据量比较大的排序应用。但是，如果需要排序的数据量很小，比如量级小于千，那么插入排序还是一个不错的选择。 插入排序在工业级库中也有着广泛的应用，在STL的sort算法和stdlib的qsort算法中，都将插入排序作为快速排序的补充，用于少量元素的排序（通常为8个或以下）。对于插入排序，如果比较操作的代价比交换操作大的话，可以采用二分查找法来减少比较操作的次数，我们称为二分插入排序，代码如下：#include <stdio.h>// 分类 -------------- 内部比较排序// 最差时间复杂度 ---- O(n^2)// 平均时间复杂度 ---- O(n^2)// 稳定性 ------------ 稳定void InsertionSortDichotomy(int A[], int n)    for (int i = 1; i < n; i++)        int get = A[i];                    // 右手抓到一张扑克牌        int right = i - 1;                // 手牌左右边界进行初始化        {            if (A[mid] > get)            else        }        {        }    }int main()    int A[] = { 5, 2, 9, 4, 7, 6, 1, 3, 8 };// 从小到大二分插入排序    InsertionSortDichotomy(A, n);    for (int i = 0; i < n; i++)        printf(\"%d \", A[i]);    printf(\"\\n\");}当n较大时，二分插入排序的比较次数比直接插入排序的最差情况好得多，但比直接插入排序的最好情况要差，所当以元素初始序列已经接近升序时，直接插入排序比二分插入排序比较次数少。二分插入排序元素移动次数与直接插入排序相同，依赖于元素初始序列。希尔排序，也叫递减增量排序，是插入排序的一种更高效的改进版本。希尔排序是不稳定的排序算法。希尔排序是基于插入排序的以下两点性质而提出改进方法的：插入排序在对几乎已经排好序的数据操作时，效率高，即可以达到线性排序的效率但插入排序一般来说是低效的，因为插入排序每次只能将数据移动一位希尔排序通过将比较的全部元素分为几个区域来提升插入排序的性能。这样可以让一个元素可以一次性地朝最终位置前进一大步。然后算法再取越来越小的步长进行排序，算法的最后一步就是普通的插入排序，但是到了这步，需排序的数据几乎是已排好的了（此时插入排序较快）。\n假设有一个很小的数据在一个已按升序排好序的数组的末端。如果用复杂度为O(n^2)的排序（冒泡排序或直接插入排序），可能会进行n次的比较和交换才能将该数据移至正确位置。而希尔排序会用较大的步长移动数据，所以小数据只需进行少数比较和交换即可到正确位置。希尔排序的代码如下：#include <stdio.h>  // 分类 -------------- 内部比较排序// 最差时间复杂度 ---- 根据步长序列的不同而不同。已知最好的为O(n(logn)^2)// 平均时间复杂度 ---- 根据步长序列的不同而不同。// 稳定性 ------------ 不稳定void ShellSort(int A[], int n)    int h = 0;    {    }    {        {            int get = A[i];            {                j = j - h;            A[j + h] = get;        h = (h - 1) / 3;                    // 递减增量}int main()    int A[] = { 5, 2, 9, 4, 7, 6, 1, 3, 8 };// 从小到大希尔排序    ShellSort(A, n);    for (int i = 0; i < n; i++)        printf(\"%d \", A[i]);    printf(\"\\n\");}以23, 10, 4, 1的步长序列进行希尔排序：希尔排序是不稳定的排序算法，虽然一次插入排序是稳定的，不会改变相同元素的相对顺序，但在不同的插入排序过程中，相同的元素可能在各自的插入排序中移动，最后其稳定性就会被打乱。比如序列：{ 3, 5, 10, 8, 7, 2, 8, 1, 20, 6 }，h=2时分成两个子序列 { 3, 10, 7, 8, 20 } 和  { 5, 8, 2, 1, 6 } ，未排序之前第二个子序列中的8在前面，现在对两个子序列进行插入排序，得到 { 3, 7, 8, 10, 20 } 和 { 1, 2, 5, 6, 8 } ，即 { 3, 1, 7, 2, 8, 5, 10, 6, 20, 8 } ，两个8的相对次序发生了改变。归并排序是创建在归并操作上的一种有效的排序算法，效率为O(nlogn)，1945年由冯·诺伊曼首次提出。归并排序的实现分为递归实现与非递归(迭代)实现。递归实现的归并排序是算法设计中分治策略的典型应用，我们将一个大问题分割成小问题分别解决，然后用所有小问题的答案来解决整个大问题。非递归(迭代)实现的归并排序首先进行是两两归并，然后四四归并，然后是八八归并，一直下去直到归并了整个数组。归并排序算法主要依赖归并(Merge)操作。归并操作指的是将两个已经排序的序列合并成一个序列的操作，归并操作步骤如下：归并排序的代码如下：#include <stdio.h>// 数据结构 ---------- 数组// 最优时间复杂度 ---- O(nlogn)// 所需辅助空间 ------ O(n)void Merge(int A[], int left, int mid, int right)// 合并两个已排好序的数组A[left...mid]和A[mid+1...right]    int len = right - left + 1;    int index = 0;    int j = mid + 1;                // 后一数组的起始元素    {    }    {    }    {    }    {    }{        return;    MergeSortRecursion(A, left, mid);    Merge(A, left, mid, right);{    for (int i = 1; i < len; i *= 2)        // 子数组的大小i初始为1，每轮翻倍        left = 0;        {            right = mid + i < len ? mid + i : len - 1;// 后一个子数组大小可能不够            left = right + 1;               // 前一个子数组索引向后移动    }{    int A2[] = { 6, 5, 3, 1, 8, 7, 2, 4 };    int n2 = sizeof(A2) / sizeof(int);    MergeSortIteration(A2, n2);                 // 非递归实现    for (int i = 0; i < n1; i++)        printf(\"%d \", A1[i]);    printf(\"\\n\");    for (int i = 0; i < n2; i++)        printf(\"%d \", A2[i]);    printf(\"\\n\");}上述代码对序列{ 6, 5, 3, 1, 8, 7, 2, 4 }进行归并排序的实例如下使用归并排序为一列数字进行排序的宏观过程：归并排序除了可以对数组进行排序，还可以高效的求出数组小和（即单调和）以及数组中的逆序对，详见这篇博文。堆排序是指利用堆这种数据结构所设计的一种选择排序算法。堆是一种近似完全二叉树的结构（通常堆是通过一维数组来实现的），并满足性质：以最大堆（也叫大根堆、大顶堆）为例，其中父结点的值总是大于它的孩子节点。我们可以很容易的定义堆排序的过程：堆排序的代码如下：#include <stdio.h>// 分类 -------------- 内部比较排序// 最差时间复杂度 ---- O(nlogn)// 平均时间复杂度 ---- O(nlogn)// 稳定性 ------------ 不稳定{    A[i] = A[j];}void Heapify(int A[], int i, int size)  // 从A[i]向下进行堆调整    int left_child = 2 * i + 1;         // 左孩子索引    int max = i;                        // 选出当前结点与其左右孩子三者之中的最大值        max = left_child;        max = right_child;    {        Heapify(A, max, size);          // 递归调用，继续从当前结点向下进行堆调整}int BuildHeap(int A[], int n)           // 建堆，时间复杂度O(n)    int heap_size = n;        Heapify(A, i, heap_size);}void HeapSort(int A[], int n)    int heap_size = BuildHeap(A, n);    // 建立一个最大堆    {        // 此处交换操作很有可能把后面元素的稳定性打乱，所以堆排序是不稳定的排序算法        Heapify(A, 0, heap_size);     // 从新的堆顶元素开始向下进行堆调整，时间复杂度O(logn)}int main()    int A[] = { 5, 2, 9, 4, 7, 6, 1, 3, 8 };// 从小到大堆排序    HeapSort(A, n);    for (int i = 0; i < n; i++)        printf(\"%d \", A[i]);    printf(\"\\n\");}堆排序算法的演示：动画中在排序过程之前简单的表现了创建堆的过程以及堆的逻辑结构。堆排序是不稳定的排序算法，不稳定发生在堆顶元素与A[i]交换的时刻。比如序列：{ 9, 5, 7, 5 }，堆顶元素是9，堆排序下一步将9和第二个5进行交换，得到序列 { 5, 5, 7, 9 }，再进行堆调整得到{ 7, 5, 5, 9 }，重复之前的操作最后得到{ 5, 5, 7, 9 }从而改变了两个5的相对次序。快速排序是由东尼·霍尔所发展的一种排序算法。在平均状况下，排序n个元素要O(nlogn)次比较。在最坏状况下则需要O(n^2)次比较，但这种状况并不常见。事实上，快速排序通常明显比其他O(nlogn)算法更快，因为它的内部循环可以在大部分的架构上很有效率地被实现出来。快速排序使用分治策略(Divide and Conquer)来把一个序列分为两个子序列。步骤为：快速排序的代码如下：#include <stdio.h>// 分类 ------------ 内部比较排序// 最差时间复杂度 ---- 每次选取的基准都是最大（或最小）的元素，导致每次只划分出了一个分区，需要进行n-1次划分才能结束递归，时间复杂度为O(n^2)// 平均时间复杂度 ---- O(nlogn)// 稳定性 ---------- 不稳定void Swap(int A[], int i, int j)    int temp = A[i];    A[j] = temp;{    int tail = left - 1;                // tail为小于基准的子数组最后一个元素的索引    {        {        }    Swap(A, tail + 1, right);           // 最后把基准放到前一个子数组的后边，剩下的子数组既是大于基准的子数组    return tail + 1;                    // 返回基准的索引{        return;    QuickSort(A, left, pivot_index - 1);}int main()    int A[] = { 5, 2, 9, 4, 7, 6, 1, 3, 8 }; // 从小到大快速排序    QuickSort(A, 0, n - 1);    for (int i = 0; i < n; i++)        printf(\"%d \", A[i]);    printf(\"\\n\");}使用快速排序法对一列数字进行排序的过程：快速排序是不稳定的排序算法，不稳定发生在基准元素与A[tail+1]交换的时刻。比如序列：{ 1, 3, 4, 2, 8, 9, 8, 7, 5 }，基准元素是5，一次划分操作后5要和第一个8进行交换，从而改变了两个元素8的相对次序。答：这是考虑到排序算法的稳定性。对于基础类型，相同值是无差别的，排序前后相同值的相对位置并不重要，所以选择更为高效的快速排序，尽管它是不稳定的排序算法；而对于非基础类型，排序前后相等实例的相对位置不宜改变，所以选择稳定的归并排序。", "author": "SteveWang", "tags": ["IT技术", "排序算法", "算法"]}
